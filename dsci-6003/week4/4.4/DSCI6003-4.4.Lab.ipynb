{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Arm Bandit\n",
    "\n",
    "**Include your code and answers in** `pair.py`.\n",
    "\n",
    "1. Fill in the code stubs in `banditstrategy.py`. The `Bandits` class (completed) is in `bandits.py`. You can simulate running the multi-arm bandit with the following code. We are creating three versions of the site with conversion rates of 5%, 3% and 6%. So hopefully we learn that the last one is the best!\n",
    "\n",
    "    ```python\n",
    "    from bandits import Bandits\n",
    "    from banditstrategy import BanditStrategy, random_choice\n",
    "\n",
    "    bandits = Bandits([0.05, 0.03, 0.06])\n",
    "    strat = BanditStrategy(bandits, random_choice)\n",
    "    strat.sample_bandits(1000)\n",
    "    print \"Number of trials:\", strat.trials\n",
    "    print \"Number of wins:\", strat.wins\n",
    "    print \"Conversion rates:\", strat.wins / strat.trials\n",
    "    print \"A total of %d wins of %d trials.\" % \\\n",
    "        (strat.wins.sum(), strat.trials.sum())\n",
    "    ```\n",
    "\n",
    "    Fill in the code stubs to implement the four strategies: epsilon-greedy, softmax, ucb1 and bayesian bandits. Take a look at the lecture notes if you need a reminder of any of the strategies.\n",
    "    \n",
    "    If you need a reminder of how any of the algorithms work, take a look at the [lecture notes](https://github.com/zipfian/multi-armed-bandit/blob/master/lecture.md#multi-arm-bandit).\n",
    "\n",
    "2. See how many wins you have of the 1000 trials using each of the six strategies (two already implemented) with the starting bandits given above.\n",
    "\n",
    "    Try running it again with all of these values and see how different each algorithm does with respect to total number of wins in 1000 rounds.\n",
    "\n",
    "    ```\n",
    "    [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "    [0.1, 0.1, 0.1, 0.1, 0.12]\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    ```\n",
    "\n",
    "3. Here is a function to calculate the regret after each iteration.\n",
    "\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    def regret(probabilities, choices):\n",
    "        '''\n",
    "        INPUT: array of floats (0 to 1), array of ints\n",
    "        OUTPUT: array of floats\n",
    "\n",
    "        Take an array of the true probabilities for each machine and an\n",
    "        array of the indices of the machine played at each round.\n",
    "        Return an array giving the total regret after each round.\n",
    "        '''\n",
    "        p_opt = np.max(probabilities)\n",
    "        return np.cumsum(p_opt - probabilities[choices])\n",
    "    ```\n",
    "\n",
    "    Make sure that you give `probabilities` as a *numpy array* and `choices` as a numpy array *of ints*.\n",
    "\n",
    "    Use matplotlib to plot the total regret over time of each algorithm. Use the Bandits with these hidden probabilities: `[0.05, 0.03, 0.06]`\n",
    "\n",
    "4. Now plot the percentage of time the optimal bandit was chosen over time.\n",
    "\n",
    "5. ***[Extra Credit]*** Experiment with the number of trials and hidden probabilities to compare the four strategies you implemented: \n",
    "    * epsilon-greedy\n",
    "    * softmax\n",
    "    * ucb1\n",
    "    * bayesian bandits\n",
    "\n",
    "Given a variety of situations (number of trials, difference in conversion rate) which one performs better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
