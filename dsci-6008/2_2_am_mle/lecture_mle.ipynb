{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum Likelihood estimation (MLE)\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"http://mattrogish.com/images/estimation/dilbert.jpeg\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Find the Maximum Likelihood estimation (MLE) for a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estimand, Estimator and Estimate, aka Statisticians are the worse at naming things\n",
    "----\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/600/1*Se8rst05XieiNBmqZ0ghBw.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estimator\n",
    "-----\n",
    "\n",
    "The Rule\n",
    "\n",
    "What we use to estimate the estimand\n",
    "\n",
    "For example:  \n",
    "$$\\bar{X} = \\frac{1}{n} \\sum_{i = 1}^n X_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is that a formula for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The estimator is a random variable and contains random variables  (not the actual observed data for them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estimand\n",
    "-----\n",
    "\n",
    "The quantity of interest\n",
    "\n",
    "What we are trying to estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What would be an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "e.g. the population mean, µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estimate\n",
    "-----\n",
    "\n",
    "The Result\n",
    "\n",
    "the actual estimate given by the estimator and a set of data\n",
    "\n",
    "e.g. the sample mean we calculate from emperical data \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Methods of Finding Estimators\n",
    "------\n",
    "\n",
    "1. Method of Moments (MOM)\n",
    "1. Maximum Likelihood Estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Method of Moments (MOM)\n",
    "-----\n",
    "\n",
    "1. Non-intuitive\n",
    "1. Complex\n",
    "1. Not used in applied settings\n",
    "\n",
    "> The method of moments ... yields consistent estimators (under very weak assumptions), though these estimators are often biased.\n",
    "\n",
    "> … maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Method_of_moments_(statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MLE: The most popular technique for deriving estimators\n",
    "------\n",
    "\n",
    "<center><img src=\"http://statgen.iop.kcl.ac.uk/media/ml1.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Likelihood function\n",
    "-----\n",
    "\n",
    "$$L(\\theta | \\textbf{x}) = L(\\theta_1, \\dots, \\theta_k | x_1, \\dots, x_n) = \\prod_{i = 1}^n f(x_i | \\theta_1, \\dots, \\theta_k) $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recall that if $X_1, \\dots, X_n$ are i.i.d. (independently, identically distributed) sample from a population with PDF \n",
    "or PMF $f(x | \\theta_1, \\dots, \\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **maximum likelihood estimator (MLE)** of the parameter $\\theta$ based on a sample $\\textbf{X}$ is $\\hat{\\theta}(\\textbf{X})$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Definition: For each sample $\\textbf{x}$, let $\\hat{\\theta}(\\textbf{x})$ be a parameter value at which $L(\\theta | \\textbf{x})$ attains its maximum as a function of $\\theta$, with $\\textbf{x}$ fixed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often $L(\\theta | \\textbf{x})$ is writen as $L(\\theta)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And $\\hat{\\theta}(\\textbf{x})$ is written as $\\hat{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/like.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/loglike.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"mle_form.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: Bernoulli MLE\n",
    "------\n",
    "\n",
    "Let $X_1, \\dots, X_n$ be i.i.d. $Bernoulli(p)$. Then the likelihood function is\n",
    "\n",
    "$$ L(p | \\textbf{x}) = \\prod_{i = 1}^n p^{x_i} (1 - p)^{1 - x_i} = p^y(1 - p)^{n - y} $$  \n",
    "\n",
    "where $y = \\sum x_i$.  \n",
    "\n",
    "[Source](http://stats.stackexchange.com/questions/181035/how-to-derive-the-likelihood-function-for-binomial-distribution-for-parameter-es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is much easier to differentiate the __log-likelihood__:  \n",
    "\n",
    "$$ l(p | \\textbf{x}) = log(L(p | \\textbf{x})) = y log(p) + (n - y) log(1 - p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differentiate log(L(p | x)) and set it to 0  \n",
    "\n",
    "$$ \\frac{y}{p} - \\frac{n - y}{1 - p} = 0 $$\n",
    "\n",
    "Solve for p, and we get,  \n",
    "\n",
    "$$\\hat{p} = \\frac{y}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The likelihood function for the coin example is:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "     L(p) &= P(X = 7) \\text{ when } X \\sim Binomial(10, p) \\\\\n",
    "          &= \\binom{10}{7} p^7 (1 - p)^{10 - 7} \\\\\n",
    "          &= \\binom{10}{7} p^7 (1 - p)^3\n",
    "   \\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Log-likelihood Function\n",
    "-----\n",
    "\n",
    "If we take the natural logarithm of the likelihood function, we get the **log-likelihood** function.  \n",
    "\n",
    "$$l(p) = log(L(p))$$\n",
    "\n",
    "For our coin example,\n",
    "\n",
    "$$ \\begin{align*}\n",
    "     l(p) &= L(p) \\\\\n",
    "          &= log\\left( \\binom{10}{7} p^7 (1 - p)^3 \\right) \\\\\n",
    "          &= log \\binom{10}{7} + 7 log(p) + 3 log(1 - p)\n",
    "   \\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: The $log \\binom{10}{7}$ is usually dropped since it is just a constant and does not affect the value that maximizes the function (we only care about the terms that involve $p$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finding the Maximum of the Log-likelihood Function  \n",
    "-----\n",
    "To find the maximizing value of $p$, we first differentiate the log-likelihood with respect to $p$:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "     \\frac{dl}{dp} &= 7 (\\frac{1}{p}) + 3 (\\frac{1}{1 - p}) \\\\\n",
    "                   &= \\frac{7}{p} + \\frac{3}{1 - p}\n",
    "   \\end{align*} $$\n",
    "   \n",
    "The maximizing value of $p$ occurs when  \n",
    "\n",
    "$$ \\frac{dl}{dp} = 0 $$\n",
    "\n",
    "This gives us  \n",
    "\n",
    "$$ \\frac{dl}{dp} = \\frac{7}{p} - \\frac{3}{1 - p} = 0 $$  \n",
    "\n",
    "$$ \\Rightarrow \\hat{p} = 0.7 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "A good estimate of the unknown parameter θ would be the value of θ that maximizes the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the likelihood of getting the data we observed.\n",
    "\n",
    "[Source](https://onlinecourses.science.psu.edu/stat414/node/191)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MLE  Formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/mle.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estimation is often Optimization\n",
    "---\n",
    "\n",
    "Finding argmax is sometimes nontrival.\n",
    "\n",
    "The MLE problem requires optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you aren't scared by math, this paper is helpful https://czep.net/stat/mlelr.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is the technique from ML2 that would help us find the argmax of the log-likelihood function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "When would we use Gradient Descent over other methods (e.g., Newton's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When is too computational expensive to inverting the Hessian directly:\n",
    "\n",
    "- Too much data\n",
    "- Too many parameters\n",
    "- Too complex of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Estimator (rule) -> Estimand (quantity of interest) -> Estimate (the result)\n",
    "- MLE is one of the best way to find the parameter of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Materials\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
