{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-Arm Bandits: Smarter A/B Testing\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"http://www.offtopia.net/ecai-2012-vago-poster/bandit.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- List the limitations of A/B testing\n",
    "- Define multi-arm bandits\n",
    "- Explain exploration / exploitation trade-off\n",
    "- Explain the following multi-arm bandits algorithms:\n",
    "    - Epsilon-Greedy Algorithm  \n",
    "    - Softmax  \n",
    "    - Upper Confidence Bound (UCB)\n",
    "    - Bayesian Bandit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Traditional A/B Testing\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/ab.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Traditional A/B Testing\n",
    "-----\n",
    "\n",
    "First, Assign equal numbers of users to Group A and Group B (100% exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second: Stop the experiment and send all the users to the more successful version of your site (100% exploitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-Arm Bandit\n",
    "-----\n",
    "\n",
    "Start exploiting better solutions before you are finished exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/b.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where does the bandit name come from?\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"http://images.fineartamerica.com/images-medium-large-5/one-arm-bandit-slot-machine-20130308-wingsdomain-art-and-photography.jpg\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-Arm Bandit\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://conversionxl.com/wp-content/uploads/2015/09/multiarmedbandit.jpg\" height=\"500\"/></center>\n",
    "\n",
    "bandits (slot machines) = versions of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multi-Arm Bandit Approach \n",
    "-----\n",
    "\n",
    "1) Show a user the version of the site that you think is best most of the time \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) As the experiment runs, update the belief about the CTR (Click Through Rate) for each version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Run for however long until satisfied we know the best version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example Scenario  \n",
    "-----\n",
    "\n",
    "The company you work for is testing out a new version of it's mobile website:\n",
    "<br>\n",
    "<center><img src=\"http://www.wordstream.com/images/mobile-ab-test-best-pratices.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After running an A/B test for an afternoon, the new version of the site appears to performing slightly better than the old version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After running the test over night, the old version of the site is performing better than the old version with a statistically significant p-value of 0.04.\n",
    "\n",
    "Do you stop the test, or do you keep running it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exploration vs. Exploitation  \n",
    "-----\n",
    "\n",
    "<center><img src=\"images/eee.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exploration**: Trying out different options to try and determine the reward associated with the given approach (i.e. acquiring more knowledge)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exploitation**: Going with the approach that you believe to have the highest expected payoff (i.e. optimizing decisions based on existing knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/ee.jpg\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Traditional A/B Testing  \n",
    "\n",
    "* A short period of *pure exploration*, in which you assign\n",
    "equal numbers of users to Group A and Group B  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A long period of *pure exploitation*, in which you send all of your users to the more successful version of your site and never come back to the option that seemed to be inferior  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "Why might this be a suboptimal strategy?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Potential Inefficiencies  \n",
    "------\n",
    "\n",
    "- Equal number of observations are routed to A and B for a preset amount of time or iterations  - Need to wait for the experiment to conclude for certain statistical guarantees to be provided  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Only after that preset amount of time or iterations do we stop and use the better performer  - This wastes time - and money - showing users the suboptimal site  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "What are examples of where we can apply bandits to reduce ineffiencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dynamic A/B Testing websites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* __Clinical trials__  (I would agrue not doing bandits is immoral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Budget allocation amongst competing projects (not a way to make friends)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Adaptive routing in attempts to minimize network delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Explore / Exploit Learning Steps\n",
    "------\n",
    "\n",
    "1. Create a prior population of hypotheses\n",
    "2. Choose a weighted random hypothesis h\n",
    "3. Act according to h and observe outcome\n",
    "4. Re-weight hypotheses\n",
    "5. Goto 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is \"Reget\"? \n",
    "------\n",
    "\n",
    "The difference of what we won and what we would expect with optimal strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can view this as a cost function we are trying to minimize  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reget formalism\n",
    "-----\n",
    "\n",
    "$$ \n",
    "\\begin{align*} \n",
    "\\text{regret} &= \\sum_{i = 1}^k (p_{opt} - p_i)   \\\\ \n",
    "&= k p_{opt} - \\sum_{i = 1}^k p_i\n",
    "\\end{align*} $$\n",
    "\n",
    "where there are k trials and $p_i$ is the probability of winning with the bandit chosen on the i-th run. $p_{opt}$ is the probability of winning with the best bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Minimizing Regret  \n",
    "-------\n",
    "\n",
    "A regret of 0 would mean you always played the best machine - this isn't ever possible since you need to collect data to determine which one is the best.\n",
    "\n",
    "Note that you need to know the true probabilities to calculate the regret. This is a theoretical idea to evaluate which algorithm is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bandit algorithms\n",
    "------\n",
    "\n",
    "1. Epsilon-Greedy Algorithm  \n",
    "2. Softmax  \n",
    "3. Upper Confidence Bound (UCB)\n",
    "4. Bayesian Bandit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/eg.jpg\" width=\"500\"/></center>\n",
    "\n",
    "Some percent of the time we explore and randomly choose one of the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The rest of the time we choose the option that has so far had the highest conversion rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Epsilon is a value between 0 and 1 which is the probability that we explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ".1 (or 10% exploration) is a standard choice for epsilon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Epsilon-Greedy Algorithm  \n",
    "\n",
    "```python\n",
    "levers = {'A', 'B', 'C'}\n",
    "\n",
    "def choose(levers):\n",
    "    if random.random() < 0.1:\n",
    "        # Exploration!\n",
    "        choice(levers)\n",
    "    else:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "    # Exploitation!\n",
    "        for lever in levers:\n",
    "            current_reward = reward(lever)\n",
    "        if current_reward > max_reward:\n",
    "            best_lever = lever\n",
    "    lever_plays[lever] +=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Epsilon-Greedy Algorithm limitations\n",
    "-------\n",
    "\n",
    "1. At the beginning, we are trying to do exploitation, but we do __not__ yet know which version is right.\n",
    "\n",
    "2. If epsilon is .10, the epsilon-greedy algorithm will eventually show the best option 90% of the time. So this means that 10% of the time the algorithm will continue to randomly show different versions of the site, no matter how confident we are that a certain version is the best. There is a point where we should stop the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Softmax  \n",
    "-----\n",
    "\n",
    "In this algorithm, we choose each version of the site in proportion to its estimated value using the following equation:\n",
    "\n",
    "Say $p_A$, $p_B$ and $p_C$ are the conversion rates for three versions of the site. We would choose site A with the following probability  \n",
    "\n",
    "$$ \\frac{exp(p_A/ \\tau)}{exp(p_A/ \\tau) + exp(p_B/ \\tau) + exp(p_C/ \\tau)} $$\n",
    "\n",
    "$\\tau$ is a choosen parameter that controls the ‘randomness’ of the choice, usually around 0.001  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Softmax Limitation\n",
    "-----\n",
    "\n",
    "It doesn’t take into account how many times it’s pulled an arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upper Confidence Bound (UCB)\n",
    "-----\n",
    "\n",
    "A class of algorithms that can be proven to have a logarithmic upper bound for regret. \n",
    "\n",
    "We choose the strategy that where the following value is the largest  \n",
    "\n",
    "$$ p_A + \\sqrt{\\frac{2 log(N)}{n_A}} $$  \n",
    "\n",
    "- where $N$ is the total number of rounds (for all sites) \n",
    "- $n_A$ is the number of times site A has been shown\n",
    "- $p_A$ is the conversion rate for that version so far  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You should first make sure to play each bandit once so none of the $n_A$'s are 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upper Confidence Bound (UCB)\n",
    "-----\n",
    "\n",
    "* UCB doesn’t use randomness at all. Unlike epsilon-greedy, it’s possible to know exactly how UCB will behave in any given situation. This can make it easier to reason about at times  \n",
    "\n",
    "* UCB doesn’t have free parameters that you need to configure before you can deploy it. This is a major improvement if you’re interested in running in the wild, because it means that you can start UCB without having clear sense of how you expect the world to behave "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian Bandit (or Thompson Sampling)\n",
    "----\n",
    "\n",
    "1. Calculate Posterior for each \"arm\" / action\n",
    "2. Pick a random sample from each Posterior\n",
    "3. Choose the arm with highest reward\n",
    "4. Update Posteriors for next round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian Bandit (or Thompson Sampling)\n",
    "----\n",
    "\n",
    "<br>\n",
    "<center><img src=\"http://fastml.com/images/ab-testing/bandits_small.png\" width=\"900\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A probability matching algorithm where we have a separate model for each of the bandits. \n",
    "\n",
    "Each bandit has an associated beta distribution with parameters:  \n",
    "\n",
    "* $\\alpha = 1 + $ number of times bandit has won\n",
    "* $\\beta = 1 + $ number of times bandit has lost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the Bayesian Beta-Binomial conjugate prior techniques used to model the click-through rate as our base model for each of the bandits.  \n",
    "\n",
    "Sample from each bandit’s distribution and play the bandit with the highest value.\n",
    "\n",
    "It will naturally converge on the bandit with the best payout.  \n",
    "\n",
    "[Source](http://fastml.com/ab-testing-with-bayesian-bandits-in-google-analytics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Demo!](https://learnforeverlearn.com/bandits/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which strategy should I use?\n",
    "------\n",
    "<center><img src=\"images/compare.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which strategy should I use?\n",
    "------\n",
    "<center><img src=\"https://image.slidesharecdn.com/mlglneai-130711133216-phpapp01/95/the-machine-learning-guide-to-fine-dining-19-638.jpg?cb=1373549630\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- In your life, you need to balance exploration (finding what you like) and exploitation (doing what you like)\n",
    "- Multi-arm bandits is a way of optimizing A/B testing by minimizing regret (not doing the best thing)\n",
    "- Epsilon-Greedy Algorithm sometimes pick randomly, other times pick the best known.\n",
    "- Softmax is structured exploration\n",
    "- UCB is optimism in the face of uncertainty.\n",
    "- Bayesian Bandit Algorithm models what we have seen (with some amount uncertainty). Randomly sample, then pick best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
