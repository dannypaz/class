{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expectation‚ÄìMaximization (EM) Algorithm  \n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"http://www.wilsonmongwe.co.za/wp-content/uploads/2015/07/400px-EM.jpg\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define Expectation‚ÄìMaximization (EM) algorithm\n",
    "- List the steps of the EM algorithm\n",
    "- Apply EM to a Data Science problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations of MLE & MAP\n",
    "-----\n",
    "\n",
    "What are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Point estimation methods.\n",
    "2. Require computation across the actual data (often there are no closed form solutions).\n",
    "3. They do __not__ estimate models with missing / incomplete data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Expectation‚ÄìMaximization (EM) algorithm?\n",
    "------\n",
    "\n",
    "An iterative method for finding maximum likelihood (MLE) or maximum a posteriori (MAP) estimates of parameters in statistical models,when the model depends on unobserved latent variables, thus can handle missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expectation‚ÄìMaximization (EM) Workflow\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://people.revoledu.com/kardi/tutorial/EM/images/EM_algorithm.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternates between performing:\n",
    "\n",
    "- __Expectation (E)__ step: Given the parameters of the current model, guess a probability distribution over completions of the missing values.\n",
    "\n",
    "- __Maximization (M)__ step: Given the ‚Äúfilled in‚Äù data, re-estimate/optimitize the parameters to update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM, more formally\n",
    "-----\n",
    "\n",
    "Alternates between performing:\n",
    "\n",
    "- __Expectation (E)__ step: Using the current estimate for the parameters, create function for the expectation of the log-likelihood.  \n",
    "\n",
    "- __Maximization (M)__ step: Computes parameters maximizing the expected log-likelihood found on the E step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The M parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM example: üê• & üê£\n",
    "------\n",
    "\n",
    "<center><img src=\"images/green.png\" width=\"500\"/></center>\n",
    "\n",
    "We have a circular problem ‚àû...\n",
    "\n",
    "If only we knew the cluster centroids, we could assign the data points to the closest clusters.   \n",
    "If only we knew which clusters the data points belong to, we could compute their centroids.  \n",
    "\n",
    "How do we assign points-to-clusters and clusters-to-points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is how we gonna solve it:\n",
    "-----\n",
    "\n",
    "1. We start by randomly placing cluster centroids.\n",
    "\n",
    "2. Then, we assign each data point to a cluster based on minimum distance.\n",
    "\n",
    "3. Then, we compute the centers of those new clusters and move the centroids to that position.\n",
    "\n",
    "4. Repeat step 2-3 until we get bored (or the centroids stop moving around)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "How is problem generally solved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://cdn.meme.am/cache/instances/folder852/75679852.jpg\" width=\"300\"/></center>\n",
    "\n",
    "__k-means__, aka the kind of clustering you should always do first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/e_1.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/m_1.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do we do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/kmeans.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source p435 of Pattern Recognition and Machine Learning by Bishop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A kinder, _softer_ k-means\n",
    "-----\n",
    "\n",
    "A ‚Äúsoft\" version of $k$-means. \n",
    "\n",
    "Instead of assigning each point to just one cluster (hard clustering), EM will attach a probability to the membership of a point in each cluster (P(cluster|point)). \n",
    "\n",
    "A data point can thus belong to several clusters (though with different probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://cdn.meme.am/instances/250x250/63984114.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM is trying to maximize the following function:\n",
    "------\n",
    "    \n",
    "<center><img src=\"images/em_form.png\" width=\"500\"/></center>\n",
    "\n",
    "- X observed variable\n",
    "- Œ∏ parameter of model\n",
    "- Z hidden or missing variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Complete data log likelihood\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/complete.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "X is directly observed.  \n",
    "Z is not directly observed.  \n",
    "\n",
    "Z is a joint (related) distribution on x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expected complete data log likelihood\n",
    "----\n",
    "<br>\n",
    "<center><img src=\"images/expected.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of the E step is to compute:\n",
    "-------\n",
    "\n",
    "Q(Œ∏, Œ∏<sup>t-1</sup>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goals of the M step is to optimize:\n",
    "------\n",
    "\n",
    "<center><img src=\"images/m.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "How do we change to from the following MLE estimate to a MAP estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/map.png\" width=\"500\"/></center> \n",
    "\n",
    "üí•üí•üí•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "When do stop alternating between E & M?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Convergence (no _signficant_ improvement)\n",
    "- We hit a priori stopping criteria\n",
    "- We run out of budget (time or money)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM Steps\n",
    "-----\n",
    "\n",
    "1. Initialize the parameters Œ∏\n",
    "2. Compute the best values for Z given Œ∏  \n",
    "3. Use the computed values of Z to compute a better estimate for the Œ∏  \n",
    "4. Iterate steps 2 and 3 until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM steps, stated another way\n",
    "-----\n",
    "\n",
    "1. Initialize the parameters of the models, either randomly or doing a \"smart seeding\"\n",
    "\n",
    "2. E Step: Find the posterior probabilities of the latent variable given current parameter values.\n",
    "\n",
    "3. M Step: Reestimate the parameter values given the current posterior probabilities.\n",
    "\n",
    "4. Repeat 2-3 monitoring the likelihood function likelihood. Hope for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What are the Z random variables in K-means?\n",
    "\n",
    "<center><img src=\"images/e_1.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The centers of the clusters.\n",
    "\n",
    "The centers of the clusters are new / missing / latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM Applications\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Anytime you want to do inference when there is missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Latent Variable modeling (Bayes Nets, Hidden Markov Modeling or Factor Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Dimension reduction (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Clustering (k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Probablistic Modeling (Topic modeling with LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Mixture Modeling (multiple processes/distributions underly a single R.V.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM algorithm tips n' tricks\n",
    "-------\n",
    "\n",
    "- Local maxima\n",
    "- Model ties\n",
    "- Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem**: ***Local maxima***\n",
    "--------\n",
    "\n",
    "EM improves P(data) at each iteration, but can get stuck in local \n",
    "maxima. \n",
    "\n",
    "There are many suboptimal parameter configurations at which EM stops because it can not improve from there. \n",
    "\n",
    "The result is __not__ the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution**: Random restarts\n",
    "\n",
    "EM 50 times or more with random initializations, and remember the model that\n",
    "got the best data likelihood. \n",
    "\n",
    "Each time you restart, you start at a different point along the curve, and hopefully eventually at one that leads to the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem**: Model Ties\n",
    "------\n",
    "\n",
    "I'm estimating a complex HMM model with many nodes. Howeveer the estimated transition options leaving from one node are equally good. The results of my EM does __not__ take a stance and just leaves all of them as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution #1**: Random restarts\n",
    "\n",
    "Another example of local min (or saddle point). Random restarts should break \"weak\"/false ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Solution #1**: Add more levels to Z Random Variable\n",
    "\n",
    "This will be break the symmetry for maximizing likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution #2**: Clever initialization\n",
    "\n",
    "For example have constrained initialization, for example [k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem**: The resulting model is ***overfitting***\n",
    "-------\n",
    "\n",
    "> ‚ÄúLook, I can predict this training data perfectly! And nothing else. . . \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution**: Use MAP estimates\n",
    "\n",
    "Add a weight to prior information (theory or historical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem**: EM is getting worse over iterations\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Solution**: Check your implementation\n",
    "--------\n",
    "\n",
    "EM is guaranteed to not get worse. However, it might get better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Is EM a supervised learning, unsupervised learning, reinforcement learning, or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Unsupervised learning__\n",
    "\n",
    "There is __no__ obvious metric of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Is EM a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__NO__. \n",
    "\n",
    "It is a way to optimize a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "When should EM __not__ be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. There is a closed form solution (for example, OLS).\n",
    "1. You have complete data (use vanilla MLE or MAP).\n",
    "1. Intractable to compute a EM step (most often it is M, updating parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Expectation‚ÄìMaximization (EM) Algorithm  is a series of steps to find good parameter estimates when there is incomplete/hidden data/variables.\n",
    "- EM steps\n",
    "    1. Initialize the parameter estimates\n",
    "    2. Given the current parameter estimates, find the min log likelihood for Z (complete data)\n",
    "    3. Given the current data, find better parameter estimates\n",
    "    3. Repeat steps 2 & 3\n",
    "- EM works well but should not be used all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/30825/what-is-the-meaning-of-the-semicolon-in-fx-theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expectation step (E step)  \n",
    "------\n",
    "\n",
    "Calculate the expected value of the log likelihood function, with respect to the conditional distribution of $Z$  given $X$ under the current estimate of the parameters $\\theta^{(t)}$:  \n",
    "\n",
    "$$ Q(\\theta | \\theta^{(t)}) = E_{Z | X, \\theta^{(t)}} \\left[ log \\text{ } L( \\theta | X, Z) \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximization step (M step)  \n",
    "----\n",
    "\n",
    "Find the parameter that maximizes this quantity:  \n",
    "\n",
    "$$ \\theta^{(t + 1)} = argmax_{\\theta} \\text{ } Q(\\theta | \\theta^{(t)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expotential Form for Sufficent Statistics\n",
    "------\n",
    "\n",
    "https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM is not so much an algorithm, but rather a class of algorithms that use a 2-step procedure learning (E step, M step) to train a ***generative model***.\n",
    "\n",
    "Generative models are joint probabilities (written $P(x,y)$) that explain how the data was, well, generated (*discriminative* models, on the other hand, are simply weight vectors that explain the conditional probability $P(x|y)$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Original Paper\n",
    "-----\n",
    "\n",
    "DEMPSTER, ARTHUR P./LAIRD, NAN M./RUBIN, DONALD B. (1977): Maximum likelihood from incomplete data via the EM algorithm. In: Journal of the Royal Statistical Society. Series B (Methodological), 39, Nr. 1, 1‚Äì38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical Models\n",
    "---\n",
    "\n",
    "Graphical Models are a nice way of visualizing probabilistic models, and also to express the dependencies\n",
    "that hold between the individual elements. There are several types of graphical models, but the\n",
    "ones we are interested in (and the ones we mean when we use the term) are ***Bayes Nets*** and ***Hidden\n",
    "Markov Models*** (HMMs). Graphical models consist of two elements, ***nodes*** and ***arcs***.\n",
    "\n",
    "The nodes are ***random variables***. Random variables are events that have some probability, and come\n",
    "in different flavors. If a random variable has exactly two values, like {`on`, `off`} or {`true`, `false`}, they\n",
    "are ***binary*** (in the latter case ***boolean***). If they have a list of values (something like {`red`, `green`, `blue`}\n",
    "or {`chocolate`, `vanilla`, `strawberry`, `pistacchio`}), they are ***discrete***. If they have numbers as values,\n",
    "they are called ***continuous***. \n",
    "The ***probabilities*** associated with each of the values of a random variable have to\n",
    "sum up to $1.0$, i.e., the variable `COLOR` with the values {`red`, `green`, `blue`} could have the respective probabilities {$0.2, 0.5, 0.3$} or\n",
    "{$0.33, 0.33, 0.33$} associated with the values, but not {$0.8, 0.4, 0.7$}.\n",
    "\n",
    "Arcs are the directed links between the random variables, and you can think of them as causal relations\n",
    "(there are other kinds, but it is easiest this way). They denote what influence the parent has on the\n",
    "child node. This influence is expressed by a conditional probability. \n",
    "\n",
    "<img src=\"images/bn1.png\" width=\"200px\"/>\n",
    "<div align=\"center\">*Figure 1: A simple graphical model with three random variables*</div>\n",
    "\n",
    "E.g., in a network like the one in Figure 1, we can say how likely it is that traffic ($T$) is bad, given that the weather ($W$) is rain. A node\n",
    "$X$ can have several parents, which means that its value is influenced by several factors (traffic could\n",
    "also be influenced by a Lakers game, $G$). If there are no links between two variables, then they are\n",
    "***independent of one another***, i.e., whether the Lakers play or not luckily has no influence on the weather\n",
    "$W$ (the examples in this section are largely influenced by [Russell/Norvig 2003](#refRN)).\n",
    "\n",
    "----\n",
    "Bayes Nets\n",
    "--\n",
    "<img src=\"images/bn2.png\" width=\"450px\"/>\n",
    "<div align=\"center\">*Figure 2: A Bayes Net with three random variables and associated parameters*</div>\n",
    "\n",
    "If we combine several nodes in a network, we call it a ***Bayes Net***. Let‚Äôs look at a very simple example\n",
    "(Figure 2), inspired by [Russell/Norvig 2003](#refRN). Say we have three random variables, namely the weather\n",
    "($W$), with values {`sunny`, `rainy`}, traffic ($T$), which can be {`normal`, `bad`, `terrible`} and whether we are\n",
    "late for a meeting ($L$: {`true`, `false`})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From pop culture we know that it never rains in southern California, and from our meterological data\n",
    "(see [section 2.1.1](#secprob)) we know that *never* means 5%. So the probabilities for $W$ are $(0.95, 0.05)$. If we\n",
    "talk about the probability of a specific outcome of the variables values, we write $P(W =$ `sunny` $) = 0.95$\n",
    "or shorter $P($ `sunny` $) = 0.95$.\n",
    "\n",
    "If it rains, traffic tends to get worse, and if traffic is bad, we are more likely to be late for our meeting.\n",
    "If it is sunny, the traffic behaves different than when it is rainy, so we have to specify the probability of\n",
    "each value of $T$ for each value of $W$. We do that in a table, where each column is a value for a variable,\n",
    "$T$ and $W$. Notice that the rows with the same value forW have to sum up to $1.0$. You can imagine that\n",
    "each value for weather is a state you are in, and the different values for $T$ are options you can choose\n",
    "from. Some options are more likely than others, but all probability is distributed between them (thus\n",
    "summing to $1.0$). You cannot choose something that is not there.\n",
    "\n",
    "Whether I am late for a meeting ($L$) in turn depends on the state of the traffic ($T$), so we have to\n",
    "specify another table with probabilities for each value of $L$ given each value of $T$. Again you can see\n",
    "that with worse traffic, our chances of being late increase.\n",
    "Using the Bayes Net, we can now compute how likely we are to be late if the weather is bad but\n",
    "traffic is normal, and other interesting things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models\n",
    "--\n",
    "\n",
    "<img src=\"images/hmm-example.png\" width=\"300px\"/>\n",
    "<div align=\"center\">*Figure 3: A Hidden Markov Model, resulting from connecting several of the Bayes Net above in a sequence*</div>\n",
    "\n",
    "Things change over time, but they might be connected. Tomorrow‚Äôs weather does not just happen, it\n",
    "actually depends on the weather of today. If we want to capture this, we can include another kind of\n",
    "conditional probabilities, namely the ones expressing how a random variable changes over time. This\n",
    "is the Markov part of HMMs. To make things easier, we assume that each state depends only on the\n",
    "previous one, not all previous states. This is one of the so-called ***Markov properties***. In order to \n",
    "make it a hidden Markov model, we assume that the random variable we are actually interested in is\n",
    "unobservable, but related to something we can observe.\n",
    "\n",
    "Using our example from above, we have the following scenario: one year from now, we want to get\n",
    "the sequence of sunny and rainy days that occurred (see Figure 3). We do not remember the weather\n",
    "($W$ is hidden), but we do have our diary, in which we noted for each day whether we were late or not\n",
    "($L$ is our observed variable, and it is dependent on $W$). We just copy the Bayes net from above for\n",
    "each day, and add the new transition probabilities $P(W_t | W_{t-1})$ between each of the Bayes nets. The\n",
    "probability means ‚Äúhow likely is it to be {`rainy`, `sunny`} today if it was {`rainy`, `sunny`} yesterday\".\n",
    "\n",
    "In this case, we do have another hidden variable, $T$, but it is not necessary for HMMs in general. In most applications (such as the tagging we will see later), you only have one hidden layer. The important\n",
    "part is that whether I am late one day does not depend on whether I was late the day before, but on the\n",
    "*weather* on that day. Also, the traffic of today is independent of yesterday‚Äôs traffic. This is why there\n",
    "are no arcs between the $T$ and $L$ variables, only the $W$ nodes. This is another Markov property, that the\n",
    "observations (here, $L$) are independent of one another. We guesstimated the probabilities $P(T|W)$ and\n",
    "$P(L|T)$ based on intuitions or data (we will later say, we *initialized* them), and we could now use EM\n",
    "to adjust them to reflect observations, using our diary as data and reconstructing the weather one year\n",
    "ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling\n",
    "--\n",
    "\n",
    "In the examples above, we have used logarithms to prevent underflow of the probabilities. That turned\n",
    "all our multiplications into additions (which is good, because it‚Äôs fast), but it also turned every addition\n",
    "into a function call to a log addition (which unfortunately is very slow). There is another way to prevent\n",
    "underflow, called ***scaling***, but it requires some more bookkeeping. The idea is that we normalize each \n",
    "column in our lattice so that it sums to 1. That way, the probabilities won‚Äôt get too small, and we can\n",
    "still use multiplication and normal addition.\n",
    "\n",
    "Practically, we add a vector to our forward-backward procedure. It has the same length as out lattice,\n",
    "and for each position of the lattice contains the sum of the forward probabilities at that position, the\n",
    "***scaling factor***. Before we move on, we normalize the column by that factor, so that it sums to 1. The \n",
    "backward pass uses the same scaling parameters as the forward pass.\n",
    "We can then use the scaled forward and backward matrices as well as the scaling factor vector to\n",
    "compute the fractional counts. See [Shen (2008)](#refShen) for more details (abstract mathematical notation, but\n",
    "very good explanations).\n",
    "\n",
    "\n",
    "Variational Bayes\n",
    "--\n",
    "\n",
    "We have earlier seen the idea of adding pseudo-counts to the fractional counts, which helps with\n",
    "smoothing. Variational Bayes inference works similarly, only that now we define a prior for our prior\n",
    "parameters.\n",
    "\n",
    "A ***prior*** is essentially a curve that looks similar to what we would like to achieve (see the example\n",
    "above of how EM‚Äôs distribution differs from the real one: a prior would look like the real one). In\n",
    "practice, the prior here is a ***Dirichlet distribution***, which takes two parameters: a probability distribution and a vector of shape parameters. Both have the same number of elements (or dimensions). If \n",
    "we have only two dimensions in each, the prior is called a Beta function. Typically, the probability\n",
    "distribution are our transition parameters, and the shape parameters are something like pseudo-counts\n",
    "for each element.\n",
    "\n",
    "This sounds rather complicated, but is relatively easy to implement. The E-step stays the same\n",
    "as before. In the M-step, the elements of the shape parameter vector are added as pseudo-counts\n",
    "to the matching fractional counts, and the result is passed through a ***Digamma function*** and finally\n",
    "exponentiated. To normalize, we add the sum of all elements in the shape vector to our denominator\n",
    "and again run it though Digamma and exponentiation. This is like a softer version of smoothing with\n",
    "pseudo-counts, where we have separate counts for each parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going Further...\n",
    "==\n",
    "\n",
    "If you want to get deeper into the matter, you could look into ***EM with features*** (Berg-Kirkpatrick et al., 2010). Instead of just using conditional probabilities, which often cannot capture \n",
    "useful properties (or only when encoded as additional states), you can add all the features you like in\n",
    "discriminative models (like word suffixes or capitalization) and still do unsupervised learning.\n",
    "\n",
    "Or explore ***structural EM***, which not only learns the parameter values, but also how many parameters there should be.\n",
    "\n",
    "If you do any of these things, or if you have questions, comments, or criticism, send me a mail‚ÄîI‚Äôd\n",
    "be dead curious to know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://github.com/dirkhovy/emtutorial/blob/master/An%20Evening%20with%20EM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
