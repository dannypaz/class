{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "wK-L Divergence\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_4.jpg\" hieght=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain K-L Divergence what is\n",
    "- Apply K-L Divergence to solve Data Science problems\n",
    "    - Machine learning model fitting\n",
    "    - Visualization t-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler (K-L) Divergence is relative entropy\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Relative entropy__ (the last kind of entropy) the \"distance\" between two probability mass functions p(x) and q(x) is defined as:\n",
    "\n",
    "<center><img src=\"images/kl_88.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler (K-L) Divergence\n",
    "------\n",
    "\n",
    "<center><img src=\"images/kl_4.jpg\" hieght=\"400\"/></center>\n",
    "\n",
    "A way of comparing two probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a baseline probability distribution p(X).\n",
    "\n",
    "And a comparsion probability distribution q(X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "How would compare the just the expected values of two probability distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Two sample t-test__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence as compression algorithm\n",
    "-----\n",
    "\n",
    "The number of __average additional bits per data point__ necessary for compression, …\n",
    "\n",
    "if we compress data in a manner that __assumes__ q(X) is the distribution underlying some data\n",
    "\n",
    "In reality, p(X) is the correct distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence in other words\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://i.stack.imgur.com/Io7JY.jpg\" width=\"500\"/></center>\n",
    "\n",
    "A measure of the information lost when Q is used to approximate P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Formula\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/kl_from.svg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Formula Redux\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_formula2.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/cross_ent.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler (K-L) Divergence Aliases\n",
    "------\n",
    "\n",
    "- Information divergence\n",
    "- Information gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The K-L Divergence is always non-negative,\n",
    "----\n",
    "<br>\n",
    "<center><img src=\"images/zero.svg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If P = Q everywhere, then D<sub>KL</sub>(P‖Q) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise there is an expected number of extra bits that most be used to identify that a value x from X coressponds to Q probability distribution than P probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Uses\n",
    "-----\n",
    "\n",
    "An evaluation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "P probability distribution (baseline) is empirical data\n",
    "\n",
    "Q probability distribution (comparsion) is a model, say a distrbution with a estimated (see next session for more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "K-L Divergence lets you estimate the probability of getting an empirical frequency distribution close from a large number of independent random variables with distribution q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence is non-symmetric\n",
    "------\n",
    "\n",
    "A non-symmetric measure of the difference between two probability distributions P and Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, comparing a theory to evidence is not the same as comparing evidence to theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__K-L Divergence is not a distance metric__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/discrete.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L divergence for parameter estimation\n",
    "-------\n",
    "\n",
    "For $\\hat{y}$ to $y$ is simply the *difference* between cross entropy and entropy:\n",
    "\n",
    "$$ \\mbox{KL}(y~||~\\hat{y})\n",
    "= \\sum_i y_i \\log \\frac{1}{\\hat{y}_i} - \\sum_i y_i \\log \\frac{1}{y_i}\n",
    "= \\sum_i y_i \\log \\frac{y_i}{\\hat{y}_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L divergence for parameter estimation\n",
    "-------\n",
    "\n",
    "It measures the number of *extra* bits we'll need on average if we encode symbols from $y$ according to $\\hat{y}$.\n",
    "\n",
    "You can think of it as a bit tax for encoding symbols from $y$ with an inappropriate distribution $\\hat{y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Note__: that minimizing cross entropy is the same as minimizing the KL divergence from $\\hat{y}$ to $y$. (They're equivalent up to an additive constant, the entropy of $y$, which doesn't depend on $\\hat{y}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/cont.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Example: Comparing 2 distributions\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/guass.png\" height=\"500\"/></center>\n",
    "\n",
    "Where is K-L Divergence zero? Why?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where is K-L Divergence highest? Why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why is sometimes positve and sometimes negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Example: Comparing many distributions\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/model.png\" height=\"500\"/></center>\n",
    "\n",
    "Black is baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ML as a minimization of KL divergence.\n",
    "------\n",
    "\n",
    "<center><img src=\"images/parameter.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence & t-SNE\n",
    "-----\n",
    "\n",
    "t-SNE uses K-L Divergence to compress representations for easier visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Notice that t-SNE does not retain distances but probabilities,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[learn more here](http://www.cs.toronto.edu/~hinton/absps/tsne.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://pbs.twimg.com/media/CEaluHhUEAA-_4V.png\" width=\"300\"/></center>\n",
    "\n",
    "If I compress word2vec to 3 dimensions from 128 with t-SNE, can I transform back to 128 dimensions later?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__NO__\n",
    "\n",
    "K-L Divergence is asymmetric. There many higher dimensions distributions that have the same relative entropy from the lower dimensions distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "When would K-L Divergence be 0 between two distributions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "iff q = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What to learn next about Information Theory\n",
    "-----\n",
    "\n",
    "- Decision Trees & Information Gain\n",
    "\n",
    "- Loss metric (e.g., binary cross entropy)\n",
    "\n",
    "- Pointwise Mutual Information (PMI)\n",
    "\n",
    "- Perplexity for language modeling in NLP\n",
    "\n",
    "- Maximum Entropy Modeling / MaxEnt\n",
    "\n",
    "- Markov chains/fields for sufficient statistics\n",
    "\n",
    "- Kolmogorov complexity for minimum message length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Kullback-Leibler divergence is mapping between two probability functions.\n",
    "- It can be used for modeling fit.\n",
    "- It is used for t-SNE visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "eg: difference in 2 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized mutual information\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/norm.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "V-Measure\n",
    "-----\n",
    "\n",
    "External entropy-based cluster evaluation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_long.png\" height=\"500\"/></center>\n",
    "\n",
    "<center><img src=\"images/kl_3.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_formula.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
