{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Beyond Just Entropy!\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"http://s2.quickmeme.com/img/77/774a6bebc9afa0af8103b46309627fa4b017278607b46849e8efb834a3f5bab2.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Write a joint and conditional probability distribution given data\n",
    "- Define and calculate the following Information Theory concepts:\n",
    "    - Joint entropy\n",
    "    - Conditional entropy\n",
    "    - Mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Search Engine Results Page (SERP)\n",
    "-----\n",
    "\n",
    "\"Clickology\" is one element of Data Science:\n",
    "\n",
    "<center><img src=\"http://searchengineland.com/figz/wp-content/seloads/2012/10/Twitter-Search-Screenshot-.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Click entropy\n",
    "-----\n",
    "\n",
    "Entropy can apply to the probability clicking.\n",
    "\n",
    "What does it mean that there is __low click entropy__ for SERP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If almost everyone clicks on that result, that query's click entropy is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What are examples of __low click entropy__ for SERP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- unambgious queries\n",
    "- high precision@k results\n",
    "- factual queries\n",
    "- trending queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What would it mean if there was high entropy for a SERP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "High entropy means clicks by people are distributed uniformly across results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__PROTIP__ If you already have good search system and still have high click entropy for some searches, personalized search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://s2.quickmeme.com/img/9a/9a1908ad84dc39afcdd2d2e03da1fe5a41dd4857e28578f87edd70e09151b911.jpg\" width=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probability Distribution: Joint & Conditional\n",
    "--------\n",
    "\n",
    "Joint probability distribution: p(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Conditional probability distribution: p(y|x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "The data: \n",
    "(x,y) - (1,0), (1,0), (2,0), (2, 1)\n",
    "\n",
    "__Joint probability distribution p(x,y)__:\n",
    "\n",
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 |   |   |\n",
    "| x=2 |   |  |\n",
    "\n",
    "\n",
    "__Conditional probability distribution p(y|x)__:\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 |  |  |\n",
    "| x=2 |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data: (x,y) - (1,0), (1,0), (2,0), (2, 1)\n",
    "\n",
    "__joint probability distribution p(x,y)__\n",
    "\n",
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1/2 | 0 |\n",
    "| x=2 | 1/4 | 1/4 |\n",
    "\n",
    "\n",
    "__conditional probability distribution p(y|x)__\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1 | 0 |\n",
    "| x=2 | 1/2 | 1/2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Joint entropy\n",
    "-----\n",
    "\n",
    "The joint entropy of two discrete random variables X and Y is the entropy of their pairing: (X, Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Joint Entropy Definition\n",
    "-----\n",
    "\n",
    "For 2 Random Variables (X, Y):\n",
    "<br><br>\n",
    "<center><img src=\"images/joint_f.svg\" width=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If X and Y are independent:\n",
    "\n",
    "X ⊥ Y then P(X, Y) = P(X)P(Y)\n",
    "\n",
    "Our uncertainty is maximal\n",
    "\n",
    "X ⊥ Y then H(X, Y) = H(X) + H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__NOTE__: You multiple independent probabilities and add entropy. The simplification of logs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general, considering events jointly reduces our uncertainty\n",
    "\n",
    "H(X, Y) ≤  H(X) + H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our joint uncertainty is ≥ our marginal uncertainty\n",
    "\n",
    "H(X, Y) ≥  H(X) ≥ H(Y) ≥ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "X(n) will be n is even.  \n",
    "Y(n) will be n is prime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| n | 1 |   2 |  3 |  4 |  5 |  6 |  7 |  8 |\n",
    "|:-------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "| X | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 |\n",
    "| Y | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "| n | 1 |   2 |  3 |  4 |  5 |  6 |  7 |  8 |\n",
    "|:-------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "| X | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 |\n",
    "| Y | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 |\n",
    "\n",
    "Fill out the joint distribution\n",
    "\n",
    "|__X__/__Y__ | 0 | 1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| 0 | ? | ? |\n",
    "| 1| ? | ? |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|__X__/__Y__ | 0 | 1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| 0 |1/8 | 3/8 |\n",
    "| 1| 3/8 | 1/8 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "|__X__/__Y__ | 0 | 1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| 0 |1/8 | 3/8 |\n",
    "| 1| 3/8 | 1/8 |  \n",
    "\n",
    "<center><img src=\"images/joint.png\" width=\"500\"/></center>\n",
    "\n",
    "What is the joint entropy of X and Y or H(X, Y)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2\n",
    "\n",
    "H_X_Y = -((1/8)*log2(1/8)+\n",
    "          (3/8)*log2(3/8)+\n",
    "          (3/8)*log2(3/8)+\n",
    "          (1/8)*log2(1/8))\n",
    "print(f\"{H_X_Y:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is H(X)?  \n",
    "What is H(X)?  \n",
    "What is H(X)+H(Y)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H(X) = 1 = (-((1/2)*log2(1/2)+(1/2)*log2(1/2))  \n",
    "H(Y) = 1 = (-((1/2)*log2(1/2)+(1/2)*log2(1/2))  \n",
    "H(X)+H(Y) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is less entropy in joint than the marginals\n",
    "\n",
    "(H(X, Y) ≈ 1.811) < (H(X) + H(Y) = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conditional entropy\n",
    "-----\n",
    "\n",
    "H(Y | X) is how much uncertainty is left in Y, once X is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__or__\n",
    "\n",
    "Quantifies the amount of information needed to describe the outcome of a random variable Y given that the value of another random variable X is known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Conditional entropy formula\n",
    "-----\n",
    "\n",
    "H(Y | X) = H(X, Y) - H(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__or__\n",
    "<br>\n",
    "<center><img src=\"images/conditional.svg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information never hurts\n",
    "------\n",
    "\n",
    "H(X, Y) ≤ H(Y)\n",
    "\n",
    "Conditioning on data decreases our uncertainty<sup>*</sup>.\n",
    "\n",
    "<sub>Almost always. Or at least never increases uncertainty. On average!</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2 Kinds of Conditional Entropy\n",
    "------\n",
    "\n",
    "Entropy can be conditioned on\n",
    "\n",
    "1. A random variable __H(Y | X)__\n",
    "2. Random variable taking a certain value __H(Y | X=x)__\n",
    "\n",
    "Care should be taken not to confuse these two definitions of conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generally the specific conditional entropy of Y given a specific value x of X is more useful:\n",
    "\n",
    "H(Y | X=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "If H(Y |X = x) = 0, then what does x tells us about Y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "x accounts for all the uncertainty of Y.\n",
    "\n",
    "If we know the value of x we know the value of Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example in English, we are mostly certain there is will be a __u__ if there is a __q__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Mutual_Information_Examples.svg\" height=\"500\"/></center>\n",
    "\n",
    "Measures the mutual dependence between the two variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information\n",
    "-----\n",
    "\n",
    "Measures the amount of information that can be obtained about one random variable by observing another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information Formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/mutual.svg\" height=\"500\"/></center>\n",
    "\n",
    "where SI (Specific mutual Information) is the pointwise mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The mutual information between random variables X and Y is a function of their joint probability mass function p(x,y) and marginal probability mass functions p(x) and p(y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information Property\n",
    "-----\n",
    "\n",
    "I(X; Y) = H(X) - H(X|Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Knowing Y, we can save an average of I(X; Y) bits in encoding X compared to __not__ knowing Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "I(X;Y) = 0 then what do we know about X and Y...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "X and Y are independent from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Rosetta Stone of IT <br>(What would DS be without Venn diagrams!)\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/diagram.png\" width=\"500\"/></center>\n",
    "\n",
    "Label each of the terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/diagram.png\" width=\"500\"/></center>\n",
    "\n",
    "- Η(X) - Entropy of X\n",
    "- Η(Y) - Entropy of X\n",
    "- Η(X,Y) - Joint Entropy\n",
    "- Η(X|Y) - Conditional entropy of X given Y\n",
    "- Η(Y|X) - Conditional entropy of Y given X\n",
    "- I(X;Y) - Mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Building from joint and conditional probability distributions, we can find related entropies.\n",
    "- Joint entropy is the entropy of their pairing.\n",
    "- Conditional entropy is the entropy of one give another.\n",
    "- Mutual information is a measure of the mutual dependence between the two variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Bonus Material\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual information is symmetric\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/sym.svg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information\n",
    "-----\n",
    "\n",
    "Mutual information is the communication rate in the presence of noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/mutual_information.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/proof.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "channel coding\n",
    "-----\n",
    "\n",
    "transmitting and storing data in a way that is robust to errors\n",
    "\n",
    "aka, error correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
