{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian Regression\n",
    "------\n",
    "\n",
    "<center><img src=\"https://i.stack.imgur.com/u5HhK.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define linear regression model\n",
    "- Explain the extension to Bayesian modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is linear regression?\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png\" height=\"500\"/></center>\n",
    "\n",
    "Am approach for modeling the __straight-line relationship__ between a response/outcome/dependent variable y and one or more explanatory/covariates/independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Simple linear regression__: only one explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Multiple linear regression__: more than one explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Linear regression is just the fancy term for finding the line of best fit. How do we measure \"best fit\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The residual sum of squares (RSS), which is the sum of the squared differences between the outputs and the linear regression estimates.\n",
    "\n",
    "We need to minimize some kind of error function. \n",
    "\n",
    "The most popular method to do this is via ordinary least squares (OLS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear Regression Formalism\n",
    "------\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i$$\n",
    "or  \n",
    "$$y = X \\beta + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data is modeled as Fit + Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$y = X \\beta + \\varepsilon$$\n",
    "\n",
    "$y = \\begin{bmatrix} y_1 \\\\y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$, \n",
    "    $X = \\begin{bmatrix} 1 & x_{11} & \\dots & x_{1p} \\\\ 1 & x_{21} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\dots & x_{np} \\end{bmatrix}$, \n",
    "    $\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}$, \n",
    "    $\\varepsilon = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}$\n",
    "\n",
    "$n$: sample size  \n",
    "$p$: number of explanatory variables / predictors  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regression models the conditional distribution\n",
    "-----\n",
    "\n",
    "p(y | θ, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fitting Linear Regression\n",
    "-----\n",
    "\n",
    "Find MLE with the Moore-Penrose pseudoinverse / Normal Equation:\n",
    "\n",
    "$$\\hat{\\beta} = ({\\bf{X^{T}X}})^{-1}{\\bf{X^{T}y}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What are the limitations of the Normal Equation (and MLE in general)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The OLS estimate of β is a point estimate and prone to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian regression formalism\n",
    "-----\n",
    "\n",
    "Given: \n",
    "\n",
    "$$y = \\beta{\\bf{x}} + {\\bf{e}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/br.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The response data points y are sampled from a multivariate normal distribution that has a mean equal to the product of the β coefficients and the predictors, X, and a variance of σ<sup>2</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Advantages of defining y from distribution\n",
    "-----\n",
    "\n",
    "1. Prior Distrubtions - in β\n",
    "2. Posterior Distrubtions - model entire distribution, larger variances mean more uncertainity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Non-informative priors\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/flat.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian regression setup\n",
    "-----\n",
    "\n",
    "However we do __not__ know β and σ<sup>2</sup>, we need to sample from p(β, σ<sup>2</sup>|y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Construct and Sample from posterior\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/sampling.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Chain Monte Carlo (MCMC)\n",
    "-------\n",
    "\n",
    "Markov Chain - memoryless  \n",
    "Monte Carlo - \"random\" search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Chain Monte Carlo (MCMC)\n",
    "-------\n",
    "\n",
    "Memoryless search performed with intelligent jumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MCMC Sampling Steps\n",
    "------\n",
    "\n",
    "1) Assume a functional form (probability distribution) for parameters θ according to hyperparameters α\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) 'Sample' this joint probability distribution to get a vector of single values for θ across d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Compute the posterior predictions ŷ for observed datapoints and compare to the ground truth y using a loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) Update the joint θ distributions according to the new density of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5) Repeat the sampling and evaluation a bunch more times (100x - 10,000x), seeking to take progressively better samples of θ according to minimising the loss function, and arriving at convergence where any new samples don't improve the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The MCMC sampling form: Markov-Chain \n",
    "-------\n",
    "\n",
    "The position of step n+1 is dependent only upon the position of step n, and otherwise independent of all other steps.\n",
    "\n",
    "Moves around the joint distribution in a semi-random manner, the distance and direction decided according tos pecific rules of the sampling method:\n",
    "\n",
    "- Gradient-seeking\n",
    "- Momentum\n",
    "- Randomness (the Monte-Carlo part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other flavors of Bayesian regression\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic\n",
    "- Poisson\n",
    "- GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://www.marketingdistillery.com/wp-content/uploads/2014/11/TheThreeRegressions.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Bayesian Logistic Regression\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/log.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Linear regression is the most popular type of analytic modeling in the world.\n",
    "- There is a logical extension to Bayesian by adding distributional weights to the parameters.\n",
    "- However often there is no longer closed-form solutions so we must numerical approximation with sampling.\n",
    "- MCMC is one convenient method of sample that take random steps that progressively approximates the posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum a posteriori (MAP) estimate of β is equivalent to ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Bayesian regression\n",
    "-----\n",
    "\n",
    "Consider the idea that the error vector ${\\bf{e}}$ is a gaussian distribution with a mean around 0. Note that the variance term $\\sigma^{2}$ is a covariance matrix:\n",
    "\n",
    "$${\\bf{e}} \\sim N(0, \\sigma^{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is a response signal with a linear mean and a variance of magnitude $\\sigma$. Now we consider the collection of data $Y=y_{i}$ paired with $X=x_{i}$, so as to make a reorderable collection of pairs $(y_{i},x_{i})$ \n",
    "\n",
    "$$p({\\bf{Y}} |\\ {\\bf{X}}, \\beta, V) = \\Pi_{i}p(y_{i}|\\ x_{i},\\beta,\\sigma^{2})$$\n",
    "\n",
    "Note that this is the product of independent probabilities. We can reform this equation in matrix form:\n",
    "\n",
    "\n",
    "$$ = \\dfrac{1}{(\\sigma^{2})^{\\frac{N}{2}}}\\text{exp}\\left(-\\frac{1}{2\\sigma^{2}}({\\bf{Y}}-\\beta{\\bf{X}})({\\bf{Y}}-\\beta{\\bf{X}})^{T}\\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Formation of the conjugate prior: The matrix-normal density\n",
    "\n",
    "Right now, our information about $\\beta$ is really vague. Rewriting an estimator for the term inside the exponential function above using the OLS (Moore-Penrose) pseudoinverse:\n",
    "\n",
    "$$\\hat{\\beta} = ({\\bf{X^{T}X}})^{-1}{\\bf{X^{T}y}}$$\n",
    "\n",
    "gives us the following expansion which is normal with respect to the error in estimate for $\\beta$, $(\\beta-\\hat{\\beta})$:\n",
    "\n",
    "$$({\\bf{Y}}-\\beta{\\bf{X}})({\\bf{Y}}-\\beta{\\bf{X}})^{T} = ({\\bf{y-\\hat{\\beta}X}})^{T}({\\bf{y-\\hat{\\beta}X}})+(\\beta-\\hat{\\beta})^{T}({\\bf{X^{T}X}})(\\beta-\\hat{\\beta})$$\n",
    "\n",
    "Now we can rewrite the likelihood:\n",
    "\n",
    "$$p({\\bf{y}}|{\\bf{X}},\\beta,\\sigma^{2}) \\propto (\\sigma^2)^{-\\frac{v}{2}} \\exp\\left(-\\frac{vs^{2}}{2{\\sigma}^{2}}\\right)(\\sigma^2)^{-\\frac{n-v}{2}} \\exp\\left(-\\frac{1}{2{\\sigma}^{2}}(\\beta - \\hat{\\beta})^{\\rm T}({\\bf{X}}^{T}{\\bf{X}})(\\beta - \\hat{\\beta})\\right)$$\n",
    "\n",
    "Given that\n",
    "\n",
    "$$vs^{2} =(\\mathbf{y}- \\hat{\\beta}\\mathbf{X} )^{\\rm T}(\\mathbf{y}- \\hat{\\beta}\\mathbf{X} ) \\quad  \\text{ and } \\quad v = n-k$$\n",
    "\n",
    "where $k$ is the number of regression coefficients.\n",
    "\n",
    "\n",
    "This suggests a form for the prior:\n",
    "\n",
    "$$p(\\beta,\\sigma^{2}) = p(\\beta|\\sigma^{2})p(\\sigma^{2})$$\n",
    "\n",
    "The conditional prior must be matrix-normal:\n",
    "\n",
    "$$p(\\beta|\\sigma^{2}) \\propto (\\sigma^2)^{-\\frac{k}{2}} \\exp\\left(-\\frac{1}{2{\\sigma}^{2}}(\\beta - \\mu_0)^{\\rm T} \\mathbf{\\Lambda}_0 (\\beta - \\mu_0)\\right)$$\n",
    "\n",
    "Where the $\\Lambda_{0}$ is a matrix of variance magnitudes (what does this remind you of)\n",
    "\n",
    "And $p(\\sigma^{2})$ is an [inverse gamma distribution](https://en.wikipedia.org/wiki/Inverse-gamma_distribution):\n",
    "\n",
    "$$p(\\sigma^{2}) \\propto \\dfrac{1}{(\\sigma^2)^{\\frac{v_{0}}{2}+1}} \\exp\\left(-\\frac{v_{0}s_{0}^{2}}{2{\\sigma}^{2}}\\right)$$\n",
    "\n",
    "In simple notation, we'd write this as the matrix normal: $N(\\mu_{0}, \\sigma^{2}\\Lambda_{0}^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Calculation of the Posterior (Evidence) for the model\n",
    "\n",
    "Using the simplifications:\n",
    "\n",
    "$a_0=\\tfrac{v_0}{2}$ and $b_0=\\tfrac{1}{2}v_0s_0^2 $ with $v_{0}$ and $s_{0}^{2}$ as the prior values of $v$ and $s^{2}$, respectively. We can write the posterior as follows:\n",
    "\n",
    "$$p(\\beta,\\sigma^{2}|\\mathbf{y},\\mathbf{X}) \\propto p(\\mathbf{y}|\\mathbf{X},\\beta,\\sigma^{2})p(\\beta|\\sigma^{2})p(\\sigma^{2}) $$\n",
    "\n",
    "$$\\propto \\dfrac{1}{(\\sigma^{2})^{\\frac{n}{2}}}\\exp\\left(-\\frac{1}{2{\\sigma}^{2}}(\\mathbf{y}- \\mathbf{X}\\beta)^{\\rm T}(\\mathbf{y}- \\mathbf{X}\\beta)\\right) \\dfrac{1}{(\\sigma^{2})^{\\frac{k}{2}}}\\exp\\left(-\\frac{1}{2{\\sigma}^{2}}(\\beta -\\mu_0)^{\\rm T} \\boldsymbol\\Lambda_0 (\\beta - \\mu_0)\\right)  \\dfrac{1}{(\\sigma^2)^{a_0+1}} \\exp\\left(-\\frac{b_0}{{\\sigma}^{2}}\\right)$$\n",
    "\n",
    "With some re-arrangement, the posterior can be re-written so that the posterior mean $\\mu_n$ of the parameter vector $\\beta$ can be expressed in terms of the least squares estimator $\\hat{\\beta}$ and the prior **coefficient** mean $\\mu_0$, with the strength of the prior indicated by the prior matrix of variances $\\Lambda_0$\n",
    "\n",
    "To justify that $\\mu_n$ is indeed the posterior **coefficient** mean, the quadratic terms in the exponential can be re-arranged in [quadratic form](https://en.wikipedia.org/wiki/Quadratic_form) about $\\beta-\\mu_n$.\n",
    "\n",
    "$(\\mathbf{y}- \\mathbf{X} \\beta)^{\\rm T}(\\mathbf{y}- \\mathbf{X} \\beta) + (\\beta - \\mu_0)^{\\rm T}\\boldsymbol\\Lambda_0(\\beta - \\mu_0) =(\\beta - \\mu_n)^{\\rm T}(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\boldsymbol\\Lambda_0)(\\beta - \\mu_n)+\\mathbf{y}^{\\rm T}\\mathbf{y}-\\mu_n^{\\rm T}(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\boldsymbol\\Lambda_0)\\mu_n+\\mu_0^{\\rm T}\\boldsymbol\\Lambda_0\\mu_0$\n",
    "\n",
    "Now we can rewrite the posterior as a normal multiplied by an inverse-gamma distribution with different coefficients:\n",
    "\n",
    "$p(\\beta,\\sigma^{2}|\\mathbf{y},\\mathbf{X}) \\propto \\dfrac{1}{(\\sigma^2)^{\\frac{k}{2}}}\\exp\\left(-\\dfrac{1}{2{\\sigma}^{2}}(\\beta - \\mu_n)^{\\rm T}(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\mathbf{\\Lambda}_0)(\\beta - \\mu_n)\\right) \\dfrac{1}{(\\sigma^2)^{\\frac{n+2a_{0}}{2}+1}}\\exp\\left(-\\dfrac{2 b_0+\\mathbf{y}^{\\rm T}\\mathbf{y}-\\mu_n^{\\rm T}(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\boldsymbol\\Lambda_0)\\mu_n+\\mu_0^{\\rm T}\\boldsymbol\\Lambda_0\\mu_0}{2{\\sigma}^{2}}\\right)$\n",
    "\n",
    "This can be interpreted as Bayesian learning where the parameters are updated according to the following equations.\n",
    "\n",
    "$ \\mu_n=(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\boldsymbol\\Lambda_0)^{-1} (\\boldsymbol\\Lambda_0\\mu_0+\\mathbf{X}^{\\rm T}\\mathbf{X}\\hat{\\beta})=(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\boldsymbol\\Lambda_0)^{-1} (\\boldsymbol\\Lambda_0\\mu_0+\\mathbf{X}^{\\rm T}\\mathbf{y})$\n",
    "\n",
    "$\\boldsymbol\\Lambda_n=(\\mathbf{X}^{\\rm T}\\mathbf{X}+\\boldsymbol\\Lambda_0)$\n",
    "\n",
    "$a_n= a_0 + \\frac{n}{2}$\n",
    "\n",
    "$b_n= b_0 + \\frac{1}{2}(\\mathbf{y}^{\\rm T}\\mathbf{y}+\\mu_0^{\\rm T}\\boldsymbol\\Lambda_0\\mu_0-\\mu_n^{\\rm T}\\boldsymbol\\Lambda_n\\mu_n) $\n",
    "\n",
    "These can be solved with Bayesian updating.\n",
    "\n",
    "The special case $\\mu_0=0$, $\\mathbf{\\Lambda}_0 = c\\mathbf{I}$ is called [(Bayesian) ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Producing a solution to the above math\n",
    "\n",
    "\n",
    "It's not difficult to see that the above model is difficult to interpret for the purpose of writing an optimization, so we are going to make some substitutions at this point to reduce the pain of finding an actual solution with a computer.\n",
    "\n",
    "There are a few different ways of getting clear solutions. \n",
    "\n",
    "### The de facto MAP estimate\n",
    "\n",
    "For simple cases, we can make a computation directly that closely resembles the OLS estimate of $\\hat{\\beta}$.\n",
    "\n",
    "Starting with an *a priori* estimate of $\\boldsymbol \\Lambda_{0}$ (this is a diagonal matrix with the inverse of each $\\lambda_{0}^{i}$ positioned on the diagonal) and $\\sigma^{2}$, we can construct the posterior directly:\n",
    "\n",
    "$\\boldsymbol \\Lambda_{0} = \\begin{bmatrix}\n",
    "                    \\frac{1}{\\lambda_{1}} & 0 & \\cdots & 0 \\\\ \n",
    "                    0 & \\frac{1}{\\lambda_{2}} & \\cdots & 0 \\\\\n",
    "\\vdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{1}{\\lambda_{n}}\n",
    "                    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\beta_{0} = \\sigma^{2}\\boldsymbol\\Lambda_{0}\\mu_{0}$\n",
    "\n",
    "An estimate of beta is formed from the above equations\n",
    "\n",
    "$\\hat{\\beta_{n}} = \\mu_{n} = (\\mathbf{X}^{\\rm T}\\mathbf{X}+\\sigma^{2}\\boldsymbol\\Lambda_{0})^{-1}(\\mathbf{X}^{\\rm T}\\mathbf{y}+\\beta_{0})$\n",
    "\n",
    "To make a prediction $\\hat{y}$, we just use the math above:\n",
    "\n",
    "$\\hat{y} = \\boldsymbol X \\hat{\\beta_{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### The Objective Estimate\n",
    "\n",
    "In professional settings, the above estimate is not often computed, and rather a computationally simpler method is used altogether.\n",
    "\n",
    "We instead make the following simplifications. First recalling back to your linear algebra, it should be obvious to you that $\\boldsymbol \\Lambda_{0}$ is a description of variance. It should also be further understood that the observed quantities ${\\bf{X}}$  (the actual data) are actually the **posterior**. We can estimate these with a QR decomposition:\n",
    "\n",
    "${\\bf X} = {\\bf QR}$\n",
    "\n",
    "$({\\bf{X}^{\\rm T}X} + \\sigma^{2} \\boldsymbol \\Lambda_{0})^{-1} \\sim ({\\bf{X}^{\\rm T}X})^{-1} = ({\\bf{QR}^{\\rm T}QR})^{-1} = ({\\bf{R}^{\\rm T}{Q}^{\\rm T}QR})^{-1} = ({\\bf{R}^{\\rm T}R})^{-1} = {\\bf R}^{\\rm -1}(\\bf R^{\\rm T})^{-1} $\n",
    "\n",
    "$(\\mathbf{X}^{\\rm T}\\mathbf{y}+\\beta_{0}) \\sim \\mathbf{X}^{\\rm T}\\mathbf{y} = {\\bf R^{\\rm T}Q^{\\rm T}}\\bf y$\n",
    "\n",
    "\n",
    "$\\hat{\\beta_{n}} = \\mu_{n} = (\\mathbf{X}^{\\rm T}\\mathbf{X}+\\sigma^{2}\\boldsymbol\\Lambda_{0})^{-1}(\\mathbf{X}^{\\rm T}\\mathbf{y}+\\beta_{0}) ={\\bf R^{-1}Q^{\\rm T}y}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26.01204318]\n",
      " [  3.24416574]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwkCXpEIY6JJUCANdkgrx/8HLHOMtdD7+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb98860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chirps = [20., 16., 19.8, 18.4, 17.1, 15.5,14.7,17.1,15.4,16.2, 15., 17.2, 16., 17., 14.1]\n",
    "temp = [88.6, 71.6, 93.3, 84.3, 80.6, 75.2, 69.7, 82., 69.4, 83.3, 78.6, 82.6, 80.6, 83.5, 76.3]\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "%matplotlib inline\n",
    "\n",
    "# now try to predict chirps with temp\n",
    "\n",
    "plt.scatter(temp, chirps)\n",
    "x = np.expand_dims(chirps,axis=1)\n",
    "ones = np.ones(shape=(len(chirps),1))\n",
    "\n",
    "X = np.hstack((ones,x)) #this is your design matrix\n",
    "n = X.shape[0]\n",
    "k = X.shape[1]\n",
    "\n",
    "y = np.expand_dims(temp, axis=1)\n",
    "\n",
    "Q, R = np.linalg.qr(X)\n",
    "RInv = np.linalg.inv(R)\n",
    "RtInv = np.linalg.inv(R.T)\n",
    "V_beta = RInv.dot(RtInv)\n",
    "Beta_hat = RInv.dot(Q.T.dot(y))\n",
    "df = n-k \n",
    "e=y-X.dot(Beta_hat) # error vector = difference in predictions\n",
    "s2 = np.sum(e.T.dot(e))/df # sum of squared errors!\n",
    "\n",
    "# Now we produce a simulation of the posterior values of the weights\n",
    "\n",
    "n_sims = 100000\n",
    "sigma = np.reshape(np.sqrt(st.invgamma.rvs(a=df/2.,scale=1./(df*s2/2.), size=n_sims)),(-1,1))\n",
    "B_sims = np.repeat(Beta_hat.T,n_sims,axis=0)+sigma*np.random.multivariate_normal(mean=np.zeros(2),cov=V_beta,size=(n_sims,))\n",
    "print(Beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngwx3SWqQ4S5JDfp/a8i9ilzivF0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb60780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B0_hist = plt.hist((B_sims[:,0]),bins=100,normed=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLkmNsNAlqREWuiQ1wkKXpEb8LxXgjFBy9KmcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119f8a908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B1_hist = plt.hist((B_sims[:,1]),bins=100,normed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, these estimates can be used to predict credibility intervals for the values of $\\beta$, and can also be used to create credibility envelopes for the the prediction (by taking the values at the edges of the credibility and using these to plot the edges of the envelope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of regressions - \n",
    "\n",
    "There are literally dozens of types of regressions that use a MAP estimate but different types of models of noise and stochasiticity in variables. These fall under the aegis of **Generalized Linear Models**. We will consider these later."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
