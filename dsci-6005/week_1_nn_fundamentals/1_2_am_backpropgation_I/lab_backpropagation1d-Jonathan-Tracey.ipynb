{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Backpropagation Lab\n",
    "\n",
    "Welcome to the backpropagation lab! By the end of this lab, you will have\n",
    "\n",
    "- Implemented forward and backward passes for linear and one-hidden layer neural network regression models with a squared loss\n",
    "- Encountered an instance of the so-called *vanishing gradient* phenomenon\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "Throughout this lab, you will be implementing forward and backward passes (i.e. backpropagation) for computational graphs (i.e. functions). To make your work easier, you will be using a particular variable naming convention, which consists of the rules\n",
    "\n",
    "- Use the exact variable names which are used in the computational graph during the forward pass\n",
    "- Use `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ in the backward pass\n",
    "\n",
    "For example, consider the function\n",
    "\n",
    "$$\n",
    "f(x, y) = \\max(3x, y)\n",
    "$$\n",
    "\n",
    "and its computational graph for the invocation $f(2, 4)$\n",
    "\n",
    "<img src=\"images/Forward Backward Example.svg\" alt=\"Forward Backward Example\" style=\"width: 600px;\"/>\n",
    "\n",
    "Using the variable naming convention above, the forward pass is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = 2, 4\n",
    "\n",
    "z = x * 3\n",
    "w = max(z, y)\n",
    "\n",
    "x, y, z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the backward pass is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dw = 1\n",
    "dy = 0 * dw\n",
    "dz = 1 * dw\n",
    "dx = 3 * dz\n",
    "\n",
    "dx, dz, dy, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable naming convention will guide your thinking and allow you to focus on the algorithm itself.\n",
    "\n",
    "---\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "A linear regression model takes the form\n",
    "\n",
    "$$\n",
    "f(x, w, b) = wx + b.\n",
    "$$\n",
    "\n",
    "for a data point $x$ and parameters $w$ and $b$. The least-squares loss function takes the form\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n",
    "$$\n",
    "\n",
    "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $f$ yields the overall loss function\n",
    "\n",
    "\\begin{align*}\n",
    "L_\\text{LR}(x, y, w, b) &= \\mathcal{L}(f(x, w, b), y) \\\\\n",
    "                        &= [f(w, x, b) - y]^2 \\\\\n",
    "                        &= [(wx + b) - y]^2\n",
    "\\end{align*}\n",
    "\n",
    "for a given $(x, y)$ training pair and parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ by hand on the computational graph $\\mathcal{G}$ = \n",
    "\n",
    "<img src=\"images/linear regression forward.svg\" alt=\"Linear Regression Forward\" style=\"width: 900px;\"/>\n",
    "\n",
    "and fill in table\n",
    "\n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $z$ | ? |\n",
    "| $\\hat{y}$ | ? |\n",
    "| $r$ | ? |\n",
    "| $\\ell$ | ? |\n",
    "\n",
    "- **Verify your answers with the instructor before continuing**\n",
    "- Write a computer program to compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ on $\\mathcal{G}$\n",
    "- Verify your forward pass is correct by comparing your two sets of values\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in $\\mathcal{G}$\n",
    "- Implement the forward pass on $\\mathcal{G}$ exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n",
    "    - Use the variable name `y_hat` for $\\hat{y}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(x,y,w,b):\n",
    "    z = x*w\n",
    "    y_hat = z+b\n",
    "    r = y_hat - y\n",
    "    del_ = r**2\n",
    "    return del_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass(3,10,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $z$ | 6 |\n",
    "| $\\hat{y}$ | 7 |\n",
    "| $r$ | -3 |\n",
    "| $\\ell$ | 9 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute gradients for all intermediate values in $\\mathcal{G}$ by hand with the values of $x$, $w$, $b$, and $y$ listed above on $\\mathcal{G}$ =\n",
    " \n",
    " <img src=\"images/linear regression backward.svg\" alt=\"Linear Regression Backward\" style=\"width: 900px;\"/>\n",
    " \n",
    " and fill the following table\n",
    " \n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $\\nabla_\\ell$ | ? |\n",
    "| $\\nabla_r$ | ? |\n",
    "| $\\nabla_{\\hat{y}}$ | ? |\n",
    "| $\\nabla_b$ | ? |\n",
    "| $\\nabla_z$ | ? |\n",
    "| $\\nabla_w$ | ? |\n",
    "\n",
    "- **Verify your answer with the instructor before continuing**\n",
    "- Compute gradients for all intermediate values in $\\mathcal{G}$ with code for the values of $x$, $y$, $w$, $b$ listed above\n",
    "- Verify the correctness of your code by comparing against the correct gradients\n",
    "\n",
    "### Hints\n",
    "\n",
    "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started\n",
    "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n",
    "- Recall the rule for chaining $\\overset{\\longleftarrow}{\\nabla}_y$ to $\\overset{\\longleftarrow}{\\nabla}_x$ when $y = f(x)$ is\n",
    "\n",
    "<img src=\"images/chain rule.svg\" alt=\"Chain Rule\" style=\"width: 300px;\"/>\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Before computing the backward pass by hand, what do you think the sign on $\\nabla{w}$ and $\\nabla{b}$ should be? How about $\\nabla_z$, $\\nabla_{\\hat{y}}$, and $\\nabla_r$? Justify your answer with intuition and not an equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla{w}$ = has a negative effect on the function. As you increase the weights, the value of the MSE decreases as you get closer to y.\n",
    "\n",
    "$\\nabla{b}$ = has a negative effect on the function. As you increase the bias, the value of the MSE decreases as you get closer to y (10)\n",
    "\n",
    "$\\nabla_z$ = has a negative effect on the function. As you increase the z, the value of the MSE decreases as you get closer to y (as long as you don't go over th evalue of y).\n",
    "\n",
    "$\\nabla_{\\hat{y}}$ = has a negative effect on the function. As you increase the weights, the value of the MSE decreases as you get closer to y ( as long as you d on't go over your value of y.\n",
    "\n",
    "$\\nabla_r$? = has a positive effect, you are  multipling this term by two (assuming this term is positive). This will be negative if R is negative. We know r is -3, so this term will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $\\nabla_\\ell$ | 1 |\n",
    "| $\\nabla_r$ | -6 |\n",
    "| $\\nabla_{\\hat{y}}$ | -6 |\n",
    "| $\\nabla_b$ | -6 |\n",
    "| $\\nabla_z$ | -6 |\n",
    "| $\\nabla_w$ | -18 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_pass(x,y,w,b):\n",
    "    r = -3\n",
    "    d_del = 1\n",
    "    d_r = 2 * r * d_del\n",
    "    d_y_hat = 1 * d_r\n",
    "    d_z = 1* d_y_hat\n",
    "    d_bias = 1 * d_y_hat\n",
    "    d_w = 3*d_z\n",
    "    \n",
    "    return d_del, d_r, d_y_hat, d_bias, d_z, d_w\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, -6, -6, -6, -6, -18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_pass(3,10,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hidden-Layer Perceptron\n",
    "\n",
    "A one-hidden-layer perceptron model takes the form\n",
    "\n",
    "$$\n",
    "g(x, w_1, b_1, w_2, b_2) = \\max(w_1 x + b_1, 0)w_2 + b_2\n",
    "$$\n",
    "\n",
    "for a data point $x$ and parameters $w_1$, $b_1$, $w_2$, $b_2$. As defined above, the least-squares loss function takes the form\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n",
    "$$\n",
    "\n",
    "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $g$ yields the loss function\n",
    "\n",
    "\\begin{align*}\n",
    "L_\\text{MLP}(x, y, w_1, b_1, w_2, b_2)\n",
    "&= \\mathcal{L}(g(x, w_1, b_1, w_2, b_2), y) \\\\\n",
    "&= [g(x, w_1, b_1, w_2, b_2) - y]^2 \\\\\n",
    "&= [(\\max(w_1 x + b_1, 0)w_2 + b_2) - y]^2\n",
    "\\end{align*}\n",
    "\n",
    "for a given $(x, y)$ training pair and parameters $w_1$, $b_1$, $w_2$, $b_2$.\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ by hand the computational graph $\\mathcal{G}$ = \n",
    "\n",
    "<img src=\"images/mlp forward.svg\" alt=\"Multi-Layer Perceptron Forward\" style=\"width: 1000px;\"/>\n",
    "\n",
    "and fill in the table\n",
    "\n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $z_1$ | ? |\n",
    "| $a$ | ? |\n",
    "| $h$ | ? |\n",
    "| $z_2$ | ? |\n",
    "| $\\hat{y}$ | ? |\n",
    "| $r$ | ? |\n",
    "| $\\ell$ | ? |\n",
    "\n",
    "- **Verify correctness of gradients with instructor before preceding**\n",
    "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ via code\n",
    "- Verify the correctness of your implementation by comparing your two sets of values\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Implement the computational graph exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Your your linear regression forward pass code as a starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $z_1$ | -2 |\n",
    "| $a$ | -1 |\n",
    "| $h$ | 0 |\n",
    "| $z_2$ | 0 |\n",
    "| $\\hat{y}$ | 1.5 |\n",
    "| $r$ | .5 |\n",
    "| $\\ell$ | .25 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".5**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute all gradients by hand on the computational graph $\\mathcal{G}$ = \n",
    "\n",
    "<img src=\"images/mlp backward.svg\" alt=\"Multi-Layer Perceptron Backward\" style=\"width: 1000px;\"/>\n",
    "\n",
    "and fill in the table\n",
    "\n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $\\nabla_\\ell$ | ? |\n",
    "| $\\nabla_r$ | ? |\n",
    "| $\\nabla_{\\hat{y}}$ | ? |\n",
    "| $\\nabla_{b_2}$ | ? |\n",
    "| $\\nabla_{z_2}$ | ? |\n",
    "| $\\nabla_{w_2}$ | ? |\n",
    "| $\\nabla_{h}$ | ? |\n",
    "| $\\nabla_{a}$ | ? |\n",
    "| $\\nabla_{b_1}$ | ? |\n",
    "| $\\nabla_{z_1}$ | ? |\n",
    "| $\\nabla_{w_1}$ | ? |\n",
    "\n",
    "- **Check with the instructor to verify your proposed solution before continuing**\n",
    "- Compute backpropagation on $\\mathcal{G}$ with code\n",
    "- Verify your two sets of answers agree\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Use your linear regression backpropagation code as a starting point\n",
    "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n",
    "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $\\nabla_\\ell$ | 1 |\n",
    "| $\\nabla_r$ | 1 |\n",
    "| $\\nabla_{\\hat{y}}$ | 1 |\n",
    "| $\\nabla_{b_2}$ | 1 |\n",
    "| $\\nabla_{z_2}$ | 1 |\n",
    "| $\\nabla_{w_2}$ | 0 |\n",
    "| $\\nabla_{h}$ | -2 |\n",
    "| $\\nabla_{a}$ | 0 |\n",
    "| $\\nabla_{b_1}$ | 0 |\n",
    "| $\\nabla_{z_1}$ | 0 |\n",
    "| $\\nabla_{w_1}$ | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_pass_two(x):\n",
    "    d_del = 1\n",
    "    r = .5\n",
    "    h = 0\n",
    "    w2 = -2\n",
    "    \n",
    "    d_r = 2*r * d_del\n",
    "    d_y_hat = 1*d_r\n",
    "    d_b2 = 1*d_y_hat\n",
    "    d_z2 = 1*d_y_hat\n",
    "    d_w2 = h *d_z2\n",
    "    d_h = w2*d_z2\n",
    "    \n",
    "    d_a = 0*d_h\n",
    "    d_b1 = 1*d_a\n",
    "    d_z1 = 1*d_a\n",
    "    d_w1 = x*d_z1\n",
    "    \n",
    "    return d_del, d_r, d_y_hat,d_b2,d_z2,d_w2 ,d_h, d_a,d_b1,d_z1,d_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.0, 1.0, 1.0, 1.0, 0.0, -2.0, -0.0, -0.0, -0.0, -0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_pass_two(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- What is the magnitude of $\\nabla_{w_1}$ as compared with $\\nabla_{w_2}$? In your own words, what does this mean? Is there a similar relationship between $\\nabla_{b_1}$ and $\\nabla_{b_2}$? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Both $\\nabla_{w_1}$ abd $\\nabla_{w_2}$ have zero impact on the function. This means that altering the weights will not impact the overall value of over function.\n",
    "\n",
    "> - $\\nabla_{b_1}$ does not have an impact on the function which means if we change the value the function value doesn't change.  $\\nabla_{b_2}$ has a positive gradient which indicates that if we increase the value here, the value of the overall function increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the function through which the gradient goes to zero? Why might this be a concern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The max function is where the gradient turns to zero. This might be worrisome because with a gradient of zero, you do not know which direction to update your weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Propose one possible solution for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Use a 'leakly' ReLU [max(0.1x,x)], so that the gradient does not turn to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tasks\n",
    "\n",
    "- Extend your one-hidden layer neural network to a three-hidden layer neural network model\n",
    "- Implement stochastic gradient descent to optimize your model on a single data point\n",
    "- Implement gradient checking to verify your backpropagation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Three hidden layer\n",
    "\n",
    "$= [\\max((\\max((\\max(w_1 x + b_1, 0)w_2 + b_2),0)w_3 + b_3),0)w_4+b_4) - y]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
