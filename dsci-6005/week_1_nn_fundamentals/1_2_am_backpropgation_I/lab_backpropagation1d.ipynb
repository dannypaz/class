{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Backpropagation Lab\n",
    "\n",
    "Welcome to the backpropagation lab! By the end of this lab, you will have\n",
    "\n",
    "- Implemented forward and backward passes for linear and one-hidden layer neural network regression models with a squared loss\n",
    "- Encountered an instance of the so-called *vanishing gradient* phenomenon\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "Throughout this lab, you will be implementing forward and backward passes (i.e. backpropagation) for computational graphs (i.e. functions). To make your work easier, you will be using a particular variable naming convention, which consists of the rules\n",
    "\n",
    "- Use the exact variable names which are used in the computational graph during the forward pass\n",
    "- Use `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ in the backward pass\n",
    "\n",
    "For example, consider the function\n",
    "\n",
    "$$\n",
    "f(x, y) = \\max(3x, y)\n",
    "$$\n",
    "\n",
    "and its computational graph for the invocation $f(2, 4)$\n",
    "\n",
    "<img src=\"images/Forward Backward Example.svg\" alt=\"Forward Backward Example\" style=\"width: 600px;\"/>\n",
    "\n",
    "Using the variable naming convention above, the forward pass is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = 2, 4\n",
    "\n",
    "z = x * 3\n",
    "w = max(z, y)\n",
    "\n",
    "x, y, z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the backward pass is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dw = 1\n",
    "dy = 0 * dw\n",
    "dz = 1 * dw\n",
    "dx = 3 * dz\n",
    "\n",
    "dx, dz, dy, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable naming convention will guide your thinking and allow you to focus on the algorithm itself.\n",
    "\n",
    "---\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "A linear regression model takes the form\n",
    "\n",
    "$$\n",
    "f(x, w, b) = wx + b.\n",
    "$$\n",
    "\n",
    "for a data point $x$ and parameters $w$ and $b$. The least-squares loss function takes the form\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n",
    "$$\n",
    "\n",
    "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $f$ yields the overall loss function\n",
    "\n",
    "\\begin{align*}\n",
    "L_\\text{LR}(x, y, w, b) &= \\mathcal{L}(f(x, w, b), y) \\\\\n",
    "                        &= [f(w, x, b) - y]^2 \\\\\n",
    "                        &= [(wx + b) - y]^2\n",
    "\\end{align*}\n",
    "\n",
    "for a given $(x, y)$ training pair and parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ by hand on the computational graph $\\mathcal{G}$ = \n",
    "\n",
    "<img src=\"images/linear regression forward.svg\" alt=\"Linear Regression Forward\" style=\"width: 900px;\"/>\n",
    "\n",
    "and fill in table\n",
    "\n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $z$ | ? |\n",
    "| $\\hat{y}$ | ? |\n",
    "| $r$ | ? |\n",
    "| $\\ell$ | ? |\n",
    "\n",
    "- **Verify your answers with the instructor before continuing**\n",
    "- Write a computer program to compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ on $\\mathcal{G}$\n",
    "- Verify your forward pass is correct by comparing your two sets of values\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in $\\mathcal{G}$\n",
    "- Implement the forward pass on $\\mathcal{G}$ exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n",
    "    - Use the variable name `y_hat` for $\\hat{y}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute gradients for all intermediate values in $\\mathcal{G}$ by hand with the values of $x$, $w$, $b$, and $y$ listed above on $\\mathcal{G}$ =\n",
    " \n",
    " <img src=\"images/linear regression backward.svg\" alt=\"Linear Regression Backward\" style=\"width: 900px;\"/>\n",
    " \n",
    " and fill the following table\n",
    " \n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $\\nabla_\\ell$ | ? |\n",
    "| $\\nabla_r$ | ? |\n",
    "| $\\nabla_{\\hat{y}}$ | ? |\n",
    "| $\\nabla_b$ | ? |\n",
    "| $\\nabla_z$ | ? |\n",
    "| $\\nabla_w$ | ? |\n",
    "\n",
    "- **Verify your answer with the instructor before continuing**\n",
    "- Compute gradients for all intermediate values in $\\mathcal{G}$ with code for the values of $x$, $y$, $w$, $b$ listed above\n",
    "- Verify the correctness of your code by comparing against the correct gradients\n",
    "\n",
    "### Hints\n",
    "\n",
    "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started\n",
    "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n",
    "- Recall the rule for chaining $\\overset{\\longleftarrow}{\\nabla}_y$ to $\\overset{\\longleftarrow}{\\nabla}_x$ when $y = f(x)$ is\n",
    "\n",
    "<img src=\"images/chain rule.svg\" alt=\"Chain Rule\" style=\"width: 300px;\"/>\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Before computing the backward pass by hand, what do you think the sign on $\\nabla{w}$ and $\\nabla{b}$ should be? How about $\\nabla_z$, $\\nabla_{\\hat{y}}$, and $\\nabla_r$? Justify your answer with intuition and not an equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hidden-Layer Perceptron\n",
    "\n",
    "A one-hidden-layer perceptron model takes the form\n",
    "\n",
    "$$\n",
    "g(x, w_1, b_1, w_2, b_2) = \\max(w_1 x + b_1, 0)w_2 + b_2\n",
    "$$\n",
    "\n",
    "for a data point $x$ and parameters $w_1$, $b_1$, $w_2$, $b_2$. As defined above, the least-squares loss function takes the form\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n",
    "$$\n",
    "\n",
    "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $g$ yields the loss function\n",
    "\n",
    "\\begin{align*}\n",
    "L_\\text{MLP}(x, y, w_1, b_1, w_2, b_2)\n",
    "&= \\mathcal{L}(g(x, w_1, b_1, w_2, b_2), y) \\\\\n",
    "&= [g(x, w_1, b_1, w_2, b_2) - y]^2 \\\\\n",
    "&= [(\\max(w_1 x + b_1, 0)w_2 + b_2) - y]^2\n",
    "\\end{align*}\n",
    "\n",
    "for a given $(x, y)$ training pair and parameters $w_1$, $b_1$, $w_2$, $b_2$.\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ by hand the computational graph $\\mathcal{G}$ = \n",
    "\n",
    "<img src=\"images/mlp forward.svg\" alt=\"Multi-Layer Perceptron Forward\" style=\"width: 1000px;\"/>\n",
    "\n",
    "and fill in the table\n",
    "\n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $z_1$ | ? |\n",
    "| $a$ | ? |\n",
    "| $h$ | ? |\n",
    "| $z_2$ | ? |\n",
    "| $\\hat{y}$ | ? |\n",
    "| $r$ | ? |\n",
    "| $\\ell$ | ? |\n",
    "\n",
    "- **Verify correctness of gradients with instructor before preceding**\n",
    "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ via code\n",
    "- Verify the correctness of your implementation by comparing your two sets of values\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Implement the computational graph exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Your your linear regression forward pass code as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute all gradients by hand on the computational graph $\\mathcal{G}$ = \n",
    "\n",
    "<img src=\"images/mlp backward.svg\" alt=\"Multi-Layer Perceptron Backward\" style=\"width: 1000px;\"/>\n",
    "\n",
    "and fill in the table\n",
    "\n",
    "| Variable | Value |\n",
    "|:--------:|:-----:|\n",
    "| $\\nabla_\\ell$ | ? |\n",
    "| $\\nabla_r$ | ? |\n",
    "| $\\nabla_{\\hat{y}}$ | ? |\n",
    "| $\\nabla_{b_2}$ | ? |\n",
    "| $\\nabla_{z_2}$ | ? |\n",
    "| $\\nabla_{w_2}$ | ? |\n",
    "| $\\nabla_{h}$ | ? |\n",
    "| $\\nabla_{a}$ | ? |\n",
    "| $\\nabla_{b_1}$ | ? |\n",
    "| $\\nabla_{z_1}$ | ? |\n",
    "| $\\nabla_{w_1}$ | ? |\n",
    "\n",
    "- **Check with the instructor to verify your proposed solution before continuing**\n",
    "- Compute backpropagation on $\\mathcal{G}$ with code\n",
    "- Verify your two sets of answers agree\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Use your linear regression backpropagation code as a starting point\n",
    "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n",
    "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- What is the magnitude of $\\nabla_{w_1}$ as compared with $\\nabla_{w_2}$? In your own words, what does this mean? Is there a similar relationship between $\\nabla_{b_1}$ and $\\nabla_{b_2}$? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the function through which the gradient goes to zero? Why might this be a concern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Propose one possible solution for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tasks\n",
    "\n",
    "- Extend your one-hidden layer neural network to a three-hidden layer neural network model\n",
    "- Implement stochastic gradient descent to optimize your model on a single data point\n",
    "- Implement gradient checking to verify your backpropagation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
