{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "version": "3.6.1", "mimetype": "text/x-python", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3"}, "anaconda-cloud": {}}, "cells": [{"metadata": {"deletable": true, "editable": true}, "source": ["# 1D Backpropagation Lab\n", "\n", "Welcome to the backpropagation lab! By the end of this lab, you will have\n", "\n", "- Implemented forward and backward passes for linear and one-hidden layer neural network regression models with a squared loss\n", "- Encountered an instance of the so-called *vanishing gradient* phenomenon\n", "\n", "Let's get started!"], "cell_type": "markdown"}, {"metadata": {"deletable": true, "editable": true}, "source": ["# Instructions\n", "\n", "Throughout this lab, you will be implementing forward and backward passes (i.e. backpropagation) for computational graphs (i.e. functions). To make your work easier, you will be using a particular variable naming convention, which consists of the rules\n", "\n", "- Use the exact variable names which are used in the computational graph during the forward pass\n", "- Use `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ in the backward pass\n", "\n", "For example, consider the function\n", "\n", "$$\n", "f(x, y) = \\max(3x, y)\n", "$$\n", "\n", "and its computational graph for the invocation $f(2, 4)$\n", "\n", "![Forward Backward Example](images/Forward%20Backward%20Example.png)", "\n", "Using the variable naming convention above, the forward pass is computed as"], "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 1, "data": {"text/plain": ["(2, 4, 6, 6)"]}}], "metadata": {"deletable": true, "editable": true, "collapsed": false}, "source": ["x, y = 2, 4\n", "\n", "z = x * 3\n", "w = max(z, y)\n", "\n", "x, y, z, w"], "cell_type": "code", "execution_count": 1}, {"metadata": {"deletable": true, "editable": true}, "source": ["and the backward pass is computed as"], "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 2, "data": {"text/plain": ["(3, 1, 0, 1)"]}}], "metadata": {"deletable": true, "editable": true, "collapsed": false}, "source": ["dw = 1\n", "dy = 0 * dw\n", "dz = 1 * dw\n", "dx = 3 * dz\n", "\n", "dx, dz, dy, dw"], "cell_type": "code", "execution_count": 2}, {"metadata": {"deletable": true, "editable": true}, "source": ["This variable naming convention will guide your thinking and allow you to focus on the algorithm itself.\n", "\n", "---\n", "\n", "# Linear Regression\n", "\n", "A linear regression model takes the form\n", "\n", "$$\n", "f(x, w, b) = wx + b.\n", "$$\n", "\n", "for a data point $x$ and parameters $w$ and $b$. The least-squares loss function takes the form\n", "\n", "$$\n", "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n", "$$\n", "\n", "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $f$ yields the overall loss function\n", "\n", "\\begin{align*}\n", "L_\\text{LR}(x, y, w, b) &= \\mathcal{L}(f(x, w, b), y) \\\\\n", "                        &= [f(w, x, b) - y]^2 \\\\\n", "                        &= [(wx + b) - y]^2\n", "\\end{align*}\n", "\n", "for a given $(x, y)$ training pair and parameters $w$ and $b$."], "cell_type": "markdown"}, {"metadata": {"deletable": true, "editable": true}, "source": ["## Forward Pass\n", "\n", "### Tasks\n", "\n", "- Compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ by hand on the computational graph $\\mathcal{G}$ = \n", "\n", "![Linear Regression Forward](images/linear%20regression%20forward.png)", "\n", "and fill in table\n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $z$ | 6 |\n", "| $\\hat{y}$ | 7 |\n", "| $r$ | -3 |\n", "| $\\ell$ | 9 |\n", "\n", "- **Verify your answers with the instructor before continuing**\n", "- Write a computer program to compute the forward pass for $L_\\text{LR}(3, 10, 2, 1)$ on $\\mathcal{G}$\n", "- Verify your forward pass is correct by comparing your two sets of values\n", "\n", "### Requirements\n", "\n", "- Use the exact variable names as used in $\\mathcal{G}$\n", "- Implement the forward pass on $\\mathcal{G}$ exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n", "    - Use the variable name `y_hat` for $\\hat{y}$ "], "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 3, "data": {"text/plain": ["(3, 2, 1, 6, 7, 10, -3, 9)"]}}], "metadata": {"deletable": true, "editable": true, "collapsed": false}, "source": ["x, y = 3, 10\n", "w, b = 2, 1\n", "\n", "z = w*x\n", "y_hat = z + b\n", "r = y_hat - y\n", "l = r**2\n", "\n", "x, w, b, z, y_hat, y, r, l"], "cell_type": "code", "execution_count": 3}, {"metadata": {"deletable": true, "editable": true}, "source": ["## Backward Pass\n", "\n", "### Tasks\n", "\n", "- Compute gradients for all intermediate values in $\\mathcal{G}$ by hand with the values of $x$, $w$, $b$, and $y$ listed above on $\\mathcal{G}$ =\n", " \n", "![Linear Regression Backward](images/linear%20regression%20backward.png)", " \n", " and fill the following table\n", " \n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $\\nabla_\\ell$ | 1 |\n", "| $\\nabla_r$ | -6 |\n", "| $\\nabla_{\\hat{y}}$ | -6 |\n", "| $\\nabla_b$ | -6 |\n", "| $\\nabla_z$ | -6 |\n", "| $\\nabla_w$ | -18 |\n", "\n", "- **Verify your answer with the instructor before continuing**\n", "- Compute gradients for all intermediate values in $\\mathcal{G}$ with code for the values of $x$, $y$, $w$, $b$ listed above\n", "- Verify the correctness of your code by comparing against the correct gradients\n", "\n", "### Hints\n", "\n", "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started\n", "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n", "- Recall the rule for chaining $\\overset{\\longleftarrow}{\\nabla}_y$ to $\\overset{\\longleftarrow}{\\nabla}_x$ when $y = f(x)$ is\n", "\n", "![Chain Rule](images/chain%20rule.png)", "\n", "### Question\n", "\n", "1. Before computing the backward pass by hand, what do you think the sign on $\\nabla{w}$ and $\\nabla{b}$ should be? How about $\\nabla_z$, $\\nabla_{\\hat{y}}$, and $\\nabla_r$? Justify your answer with intuition and not an equation.\n", "\n", "### Answer\n", "\n", "1. $\\nabla_w$ should be negative because increasing $w$ a little bit will result in a small increase in $\\hat{y}$, which will result in a small decrease in $r$, which will result in a small decrease in $\\ell$. Similar reasoning holds for same reasoning holds for $\\nabla_b$, $\\nabla_{\\hat{y}}$, and $\\nabla_r$."], "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 8, "data": {"text/plain": ["(-18, -6, -6, -6, -6, 1)"]}}], "metadata": {"deletable": true, "editable": true, "collapsed": false}, "source": ["dl = 1\n", "dr = (2*r) * dl\n", "dy_hat = 1 * dr\n", "dz, db = 1*dy_hat, 1*dy_hat\n", "dw = x * dz\n", "\n", "dw, db, dz, dy_hat, dr, dl"], "cell_type": "code", "execution_count": 8}, {"metadata": {"deletable": true, "editable": true}, "source": ["# One-Hidden-Layer Perceptron\n", "\n", "A one-hidden-layer perceptron model takes the form\n", "\n", "$$\n", "g(x, w_1, b_1, w_2, b_2) = \\max(w_1 x + b_1, 0)w_2 + b_2\n", "$$\n", "\n", "for a data point $x$ and parameters $w_1$, $b_1$, $w_2$, $b_2$. As defined above, the least-squares loss function takes the form\n", "\n", "$$\n", "\\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2.\n", "$$\n", "\n", "for predicted $\\hat{y}$ and true target $y$. Applying $L$ to $g$ yields the loss function\n", "\n", "\\begin{align*}\n", "L_\\text{MLP}(x, y, w_1, b_1, w_2, b_2)\n", "&= \\mathcal{L}(g(x, w_1, b_1, w_2, b_2), y) \\\\\n", "&= [g(x, w_1, b_1, w_2, b_2) - y]^2 \\\\\n", "&= [(\\max(w_1 x + b_1, 0)w_2 + b_2) - y]^2\n", "\\end{align*}\n", "\n", "for a given $(x, y)$ training pair and parameters $w_1$, $b_1$, $w_2$, $b_2$.\n", "\n", "## Forward Pass\n", "\n", "### Tasks\n", "\n", "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ by hand the computational graph $\\mathcal{G}$ = \n", "\n", "![Multi-Layer Perceptron Forward](images/mlp%20forward.png)", "\n", "and fill in the table\n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $z_1$ | -2 |\n", "| $a$ | -1 |\n", "| $h$ | 0 |\n", "| $z_2$ | 0 |\n", "| $\\hat{y}$ | 1.5 |\n", "| $r$ | 0.5 |\n", "| $\\ell$ | 0.25 |\n", "\n", "- **Verify correctness of gradients with instructor before preceding**\n", "- Compute the forward pass for $L_\\text{MLP}(2, 1, -1, 1, -2, 1.5)$ via code\n", "- Verify the correctness of your implementation by comparing your two sets of values\n", "\n", "### Requirements\n", "\n", "- Use the exact variable names as used in the computational graph\n", "- Implement the computational graph exactly as shown and do not skip steps (e.g. merging multiple boxes into one operation)\n", "\n", "### Hints\n", "\n", "- Your your linear regression forward pass code as a starting point"], "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 9, "data": {"text/plain": ["(2, -1, -2, 1, -1, 0, -2, 0, 1.5, 1.5, 1, 0.5, 0.25)"]}}], "metadata": {"deletable": true, "editable": true, "collapsed": false}, "source": ["x, y = 2, 1\n", "w1, b1, w2, b2 = -1, 1, -2, 1.5\n", "\n", "z1 = w1 * x\n", "a = z1 + b1\n", "h = max(a, 0)\n", "z2 = w2 * h\n", "y_hat = z2 + b2\n", "r = y_hat - y\n", "l = r**2\n", "\n", "x, w1, z1, b1, a, h, w2, z2, b2, y_hat, y, r, l"], "cell_type": "code", "execution_count": 9}, {"metadata": {"deletable": true, "editable": true}, "source": ["## Backward Pass\n", "\n", "### Tasks\n", "\n", "- Compute all gradients by hand on the computational graph $\\mathcal{G}$ = \n", "\n", "![Multi-Layer Perceptron Backward](images/mlp%20backward.png)", "\n", "and fill in the table\n", "\n", "| Variable | Value |\n", "|:--------:|:-----:|\n", "| $\\nabla_\\ell$ | 1 |\n", "| $\\nabla_r$ | 1 |\n", "| $\\nabla_{\\hat{y}}$ | 1 |\n", "| $\\nabla_{b_2}$ | 1 |\n", "| $\\nabla_{z_2}$ | 1 |\n", "| $\\nabla_{w_2}$ | 0 |\n", "| $\\nabla_{h}$ | -2 |\n", "| $\\nabla_{a}$ | 0 |\n", "| $\\nabla_{b_1}$ | 0 |\n", "| $\\nabla_{z_1}$ | 0 |\n", "| $\\nabla_{w_1}$ | 0 |\n", "\n", "- **Check with the instructor to verify your proposed solution before continuing**\n", "- Compute backpropagation on $\\mathcal{G}$ with code\n", "- Verify your two sets of answers agree\n", "\n", "### Requirements\n", "\n", "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n", "\n", "### Hints\n", "\n", "- Use your linear regression backpropagation code as a starting point\n", "- Reference the intermediate variables you computed above in the forward pass for computing all local gradients\n", "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started"], "cell_type": "markdown"}, {"outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 10, "data": {"text/plain": ["(1, 1.0, 1.0, 1.0, 1.0, -2.0, -0.0, -0.0, -0.0, -0.0)"]}}], "metadata": {"deletable": true, "editable": true, "collapsed": false}, "source": ["dl = 1\n", "dr = 2*r * dl\n", "dy_hat = 1 * dr\n", "dz2, db2 = 1*dy_hat, 1*dy_hat\n", "dh = w2 * dz2\n", "da = (a > 0) * dh\n", "dz1, db1 = 1*da, 1*da\n", "dw1 = x * dz1\n", "\n", "dl, dr, dy_hat, db2, dz2, dh, da, dz1, db1, dw1"], "cell_type": "code", "execution_count": 10}, {"metadata": {"deletable": true, "editable": true}, "source": ["### Question\n", "\n", "- What is the magnitude of $\\nabla_{w_1}$ as compared with $\\nabla_{w_2}$? In your own words, what does this mean? Is there a similar relationship between $\\nabla_{b_1}$ and $\\nabla_{b_2}$? If so, why?\n", "\n", "### Answer\n", "\n", "- The magnitudes of $\\nabla_{w_1}$ and $\\nabla_{w_2}$ are the same. The magnitude of $\\nabla_{b_2}$ is greater than the magnitude of $\\nabla_{b_1}$ (which is 0). This means that $b_2$ can influence the loss, but $b_1$ cannot so we should only concern ourselves with $b_1$ for this data point.\n", "\n", "### Question\n", "\n", "- What is the function through which the gradient goes to zero? Why might this be a concern?\n", "\n", "### Answer\n", "\n", "- The ReLU function. It is concerning if we have many layers in a neural network before a ReLU which outputs a 0 (in this example we only have a single layer). If this happens frequently enough the effective size of our model will be greatly diminished, along with its capactiy and ability to learn complex features.\n", "\n", "### Question\n", "\n", "- What is one possible solution for to this problem?\n", "\n", "### Answer\n", "\n", "- Sample different data points\n", "- Initialize each bias term with a positive value\n", "- Use a different initialization strategy for $w_i$\n", "\n", "### Bonus Tasks\n", "\n", "- Extend your one-hidden layer neural network to a three-hidden layer neural network model\n", "- Implement stochastic gradient descent to optimize your model on a single data point\n", "- Implement gradient checking to verify your backpropagation code"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"deletable": true, "editable": true, "collapsed": true}, "source": [], "cell_type": "code", "execution_count": null}], "nbformat_minor": 1, "nbformat": 4}