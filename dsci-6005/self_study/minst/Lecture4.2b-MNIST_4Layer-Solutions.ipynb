{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class coding exercises\n",
    "Replace plain vanilla gradient descent with a different accelerated gradient method - AdaDelt, NAG or AdaGrad.  \n",
    "Add dropout for regularization - What meta parameters does dropout add?  \n",
    "Add a L2 weight penalty for regularization.  What meta parameters does weight penalty add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6841\n",
      "5 0.9093\n",
      "10 0.9405\n",
      "15 0.9515\n",
      "20 0.9554\n",
      "25 0.9583\n",
      "30 0.9594\n"
     ]
    }
   ],
   "source": [
    "# 4-layer MNIST with momentum with batch normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from math import sqrt\n",
    "\n",
    "#build and initialize weights\n",
    "def init_weights(shape, name, glorot=False):\n",
    "    [n_inputs, n_outputs] = shape\n",
    "    init_range = sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    if glorot: return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "    else: return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "def momentum(list_of_weights, list_of_gradients, beta, delta):\n",
    "    # INPUT:\n",
    "    # beta is what percent of accumulated gradient you want to keep: like 0.85\n",
    "    # delta is your learning rate: like 0.01\n",
    "    \n",
    "    # OUTPUT:\n",
    "    # outputs the updaters for an accumulated step for layer's weights and the weights themselves. \n",
    "    list_of_steps = [tf.Variable(tf.zeros(weight.get_shape(), dtype=tf.float32), name='step') for weight\n",
    "                     in list_of_weights]\n",
    "    udStep = [step.assign(beta * step - (1 - beta) * delta * grad) for step, grad in \n",
    "                 zip(list_of_steps, list_of_gradients)]\n",
    "    udWt = [weight.assign(weight + step) for weight, step in zip(list_of_weights, udStep)]\n",
    "    return udStep + udWt\n",
    "\n",
    "\n",
    "#read in the data and run through training\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    lr = tf.constant(0.00002, dtype=tf.float32, name='lr')\n",
    "\n",
    "    w1 = init_weights([784, 300], 'w1')\n",
    "    w2 = init_weights([300, 50], 'w2')\n",
    "    w3 = init_weights((50, 10), 'w3')\n",
    "\n",
    "    #define network\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1))  #look under Neural Net -> Activation in API left column\n",
    "    h1_bn = tf.contrib.layers.batch_norm(h1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1_bn, w2))\n",
    "    h2_bn = tf.contrib.layers.batch_norm(h2)\n",
    "    logits = tf.matmul(h2_bn, w3)\n",
    "    py_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(py_x, dimension=1)\n",
    "\n",
    "    #define cost\n",
    "    rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='rows_of_cost')\n",
    "    cost = tf.reduce_mean(rows_of_cost, reduction_indices=None, keep_dims=False, name='cost')\n",
    "\n",
    "    #start building list that you'll reference in sess.run\n",
    "    udList = [cost]\n",
    "\n",
    "    #use hand-crafted updater\n",
    "    W = [w1, w2, w3]\n",
    "\n",
    "    #calculate gradients\n",
    "    grad = tf.gradients(cost, W)\n",
    "\n",
    "    # momentum\n",
    "    udList = udList + momentum(W, grad, 0.85, 0.001) # hyper-parameter 0.85 and 0.001 can be placeholders\n",
    "\n",
    "    #form a list of the updates - including this in sess.run will force calculation of new weights each step\n",
    "    # udList = udList + [w.assign(w - lr * g) for (w, g) in zip(W, grad)]\n",
    "\n",
    "    #use tf.optimizer by uncommenting the following two lines (and modifying where necessary)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    #train = optimizer.minimize(cost)\n",
    "\n",
    "    #output for tensorboard\n",
    "    summary1 = tf.scalar_summary(\"Cost over time\", cost) \n",
    "    summary2 = tf.histogram_summary('Weight w1 over time', w1)\n",
    "    summary3 = tf.histogram_summary('Weight w2 over time', w2)\n",
    "    summary4 = tf.histogram_summary('Weight w3 over time', w3)\n",
    "    merged = tf.merge_summary([summary1, summary2, summary3, summary4]) \n",
    "\n",
    "    #add tensorboard output to sess.run list\n",
    "    udList.append(merged)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    result = sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    miniBatchSize = 40\n",
    "    startEnd = zip(range(0, len(xTest), miniBatchSize), range(miniBatchSize, len(xTest) + 1, miniBatchSize))\n",
    "    costList = []\n",
    "    nPasses = 31\n",
    "    iteration = 0\n",
    "    for iPass in range(nPasses):\n",
    "        for (s, e) in startEnd:\n",
    "            [costVal, step1, step2, step3, weight1, weight2, weight3, tbSummary] = sess.run(udList, \n",
    "                 feed_dict={X: xTrain[s:e,], Y: yTrain[s:e]})\n",
    "            \n",
    "            writer.add_summary(tbSummary, iteration)\n",
    "            iteration += 1\n",
    "            costList.append(costVal)\n",
    "        if iPass % 5 == 0: \n",
    "            testResult = sess.run([y_pred], feed_dict={X:xTest})\n",
    "            print iPass, np.mean(np.argmax(yTest, axis=1) == testResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0857\n",
      "5 0.111\n",
      "10 0.1372\n",
      "15 0.1602\n",
      "20 0.1796\n",
      "25 0.1984\n",
      "30 0.2221\n"
     ]
    }
   ],
   "source": [
    "# 4-layer MNIST with momentum and dropout\n",
    "\n",
    "# all you do is:\n",
    "# 1) for each layer, put it in tf.nn.dropout(layer, percent_keep)\n",
    "# 2) add a percent_keep placeholder (can be a placeholder, so you can \n",
    "# adjust it in session.run\n",
    "# 3) modify sess.run() to put in fill in placeholder in feed_dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from math import sqrt\n",
    "\n",
    "#build and initialize weights\n",
    "def init_weights(shape, name, glorot=False):\n",
    "    [n_inputs, n_outputs] = shape\n",
    "    init_range = sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    if glorot: return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "    else: return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "def momentum(list_of_weights, list_of_gradients, beta, delta):\n",
    "    # INPUT:\n",
    "    # beta is what percent of accumulated gradient you want to keep: like 0.85\n",
    "    # delta is your learning rate: like 0.01\n",
    "    \n",
    "    # OUTPUT:\n",
    "    # outputs the updaters for an accumulated step for layer's weights and the weights themselves. \n",
    "    list_of_steps = [tf.Variable(tf.zeros(weight.get_shape(), dtype=tf.float32), name='step') for weight\n",
    "                     in list_of_weights]\n",
    "    udStep = [step.assign(beta * step - (1 - beta) * delta * grad) for step, grad in \n",
    "                 zip(list_of_steps, list_of_gradients)]\n",
    "    udWt = [weight.assign(weight + step) for weight, step in zip(list_of_weights, udStep)]\n",
    "    return udStep + udWt\n",
    "\n",
    "\n",
    "#read in the data and run through training\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    lr = tf.constant(0.00002, dtype=tf.float32, name='lr')\n",
    "\n",
    "    w1 = init_weights([784, 300], 'w1')\n",
    "    w2 = init_weights([300, 50], 'w2')\n",
    "    w3 = init_weights((50, 10), 'w3')\n",
    "\n",
    "    #define network\n",
    "    percent_keep = tf.placeholder(tf.float32) # add this line\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1))\n",
    "    h1 = tf.nn.dropout(h1, percent_keep) # add this line\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
    "    h2 = tf.nn.dropout(h2, percent_keep) # add this line\n",
    "    logits = tf.matmul(h2, w3)\n",
    "    py_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(py_x, dimension=1)\n",
    "\n",
    "    #define cost\n",
    "    rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='rows_of_cost')\n",
    "    cost = tf.reduce_mean(rows_of_cost, reduction_indices=None, keep_dims=False, name='cost')\n",
    "\n",
    "    #start building list that you'll reference in sess.run\n",
    "    udList = [cost]\n",
    "\n",
    "    #use hand-crafted updater\n",
    "    W = [w1, w2, w3]\n",
    "\n",
    "    #calculate gradients\n",
    "    grad = tf.gradients(cost, W)\n",
    "\n",
    "    # momentum\n",
    "    udList = udList + momentum(W, grad, 0.85, 0.001)\n",
    "\n",
    "    #form a list of the updates - including this in sess.run will force calculation of new weights each step\n",
    "    # udList = udList + [w.assign(w - lr * g) for (w, g) in zip(W, grad)]\n",
    "\n",
    "    #use tf.optimizer by uncommenting the following two lines (and modifying where necessary)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    #train = optimizer.minimize(cost)\n",
    "\n",
    "    #output for tensorboard\n",
    "    summary1 = tf.scalar_summary(\"Cost over time\", cost) \n",
    "    summary2 = tf.histogram_summary('Weight w1 over time', w1)\n",
    "    summary3 = tf.histogram_summary('Weight w2 over time', w2)\n",
    "    summary4 = tf.histogram_summary('Weight w3 over time', w3)\n",
    "    merged = tf.merge_summary([summary1, summary2, summary3, summary4]) \n",
    "\n",
    "    #add tensorboard output to sess.run list\n",
    "    udList.append(merged)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    result = sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    miniBatchSize = 40\n",
    "    startEnd = zip(range(0, len(xTest), miniBatchSize), range(miniBatchSize, len(xTest) + 1, miniBatchSize))\n",
    "    costList = []\n",
    "    nPasses = 31\n",
    "    iteration = 0\n",
    "    for iPass in range(nPasses):\n",
    "        for (s, e) in startEnd:\n",
    "            [costVal, step1, step2, step3, weight1, weight2, weight3, tbSummary] = sess.run(\n",
    "                udList, feed_dict={X: xTrain[s:e,], Y: yTrain[s:e], percent_keep: 0.5})\n",
    "            \n",
    "            writer.add_summary(tbSummary, iteration)\n",
    "            iteration += 1\n",
    "            costList.append(costVal)\n",
    "        if iPass % 5 == 0: \n",
    "            testResult = sess.run([y_pred], feed_dict={X:xTest, percent_keep: 1.0})\n",
    "            print iPass, np.mean(np.argmax(yTest, axis=1) == testResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0835\n",
      "5 0.1283\n",
      "10 0.182\n",
      "15 0.2287\n",
      "20 0.268\n",
      "25 0.3011\n",
      "30 0.3268\n"
     ]
    }
   ],
   "source": [
    "# 4-layer MNIST with momentum and L2 regularization\n",
    "\n",
    "# all you do is:\n",
    "# 1) add a penalty variable (can be a placeholder, so you can \n",
    "# adjust it in session.run\n",
    "# 2) modify the cost function to subtract weight matrices squared\n",
    "# 3) modify sess.run() to put in fill in placeholder in feed_dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from math import sqrt\n",
    "\n",
    "#build and initialize weights\n",
    "def init_weights(shape, name, glorot=False):\n",
    "    [n_inputs, n_outputs] = shape\n",
    "    init_range = sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    if glorot: return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "    else: return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "def momentum(list_of_weights, list_of_gradients, beta, delta):\n",
    "    # INPUT:\n",
    "    # beta is what percent of accumulated gradient you want to keep: like 0.85\n",
    "    # delta is your learning rate: like 0.01\n",
    "    \n",
    "    # OUTPUT:\n",
    "    # outputs the updaters for an accumulated step for layer's weights and the weights themselves. \n",
    "    list_of_steps = [tf.Variable(tf.zeros(weight.get_shape(), dtype=tf.float32), name='step') for weight\n",
    "                     in list_of_weights]\n",
    "    udStep = [step.assign(beta * step - (1 - beta) * delta * grad) for step, grad in \n",
    "                 zip(list_of_steps, list_of_gradients)]\n",
    "    udWt = [weight.assign(weight + step) for weight, step in zip(list_of_weights, udStep)]\n",
    "    return udStep + udWt\n",
    "\n",
    "\n",
    "#read in the data and run through training\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    lr = tf.constant(0.00002, dtype=tf.float32, name='lr')\n",
    "\n",
    "    w1 = init_weights([784, 300], 'w1')\n",
    "    w2 = init_weights([300, 50], 'w2')\n",
    "    w3 = init_weights((50, 10), 'w3')\n",
    "\n",
    "    #define network\n",
    "    percent_keep = tf.placeholder(tf.float32) # add this line\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
    "    logits = tf.matmul(h2, w3)\n",
    "    py_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(py_x, dimension=1)\n",
    "\n",
    "    #define cost\n",
    "    penalty = tf.placeholder(tf.float32) # penalty for L2 regularization\n",
    "    rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='rows_of_cost')\n",
    "    cost = tf.reduce_mean(rows_of_cost, reduction_indices=None, keep_dims=False, \n",
    "                          name='cost') - penalty * (tf.reduce_sum(tf.square(w1)) + \n",
    "                          tf.reduce_sum(tf.square(w2)))\n",
    "\n",
    "\n",
    "    #start building list that you'll reference in sess.run\n",
    "    udList = [cost]\n",
    "\n",
    "    #use hand-crafted updater\n",
    "    W = [w1, w2, w3]\n",
    "\n",
    "    #calculate gradients\n",
    "    grad = tf.gradients(cost, W)\n",
    "\n",
    "    # momentum\n",
    "    udList = udList + momentum(W, grad, 0.85, 0.001)\n",
    "\n",
    "    #form a list of the updates - including this in sess.run will force calculation of new weights each step\n",
    "    # udList = udList + [w.assign(w - lr * g) for (w, g) in zip(W, grad)]\n",
    "\n",
    "    #use tf.optimizer by uncommenting the following two lines (and modifying where necessary)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    #train = optimizer.minimize(cost)\n",
    "\n",
    "    #output for tensorboard\n",
    "    summary1 = tf.scalar_summary(\"Cost over time\", cost) \n",
    "    summary2 = tf.histogram_summary('Weight w1 over time', w1)\n",
    "    summary3 = tf.histogram_summary('Weight w2 over time', w2)\n",
    "    summary4 = tf.histogram_summary('Weight w3 over time', w3)\n",
    "    merged = tf.merge_summary([summary1, summary2, summary3, summary4]) \n",
    "\n",
    "    #add tensorboard output to sess.run list\n",
    "    udList.append(merged)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    result = sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    miniBatchSize = 40\n",
    "    startEnd = zip(range(0, len(xTest), miniBatchSize), range(miniBatchSize, len(xTest) + 1, miniBatchSize))\n",
    "    costList = []\n",
    "    nPasses = 31\n",
    "    iteration = 0\n",
    "    for iPass in range(nPasses):\n",
    "        for (s, e) in startEnd:\n",
    "            [cost, step1, step2, step3, weight1, weight2, weight3, tbSummary] = sess.run(\n",
    "                udList, feed_dict={X: xTrain[s:e,], Y: yTrain[s:e], penalty: 0.0001})\n",
    "            \n",
    "            writer.add_summary(tbSummary, iteration)\n",
    "            iteration += 1\n",
    "            costList.append(cost)\n",
    "        if iPass % 5 == 0: \n",
    "            testResult = sess.run([y_pred], feed_dict={X:xTest, penalty: 0.0001})\n",
    "            print iPass, np.mean(np.argmax(yTest, axis=1) == testResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Exercise\n",
    "Build 4-layer network for classifying Cifar images.  Use 10k training data (as in last lecture) to truncate the training time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2511\n",
      "5 0.3465\n",
      "10 0.3646\n",
      "15 0.3738\n",
      "20 0.395\n",
      "25 0.3917\n",
      "30 0.4027\n"
     ]
    }
   ],
   "source": [
    "# 4-layer cifar using dropout\n",
    "# quite a lot of things to change to get it to run\n",
    "\n",
    "# switched test and training set to train faster\n",
    "# used original training set as test set to print accuracy per 5 epochs\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cifarHandler import cifar\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "xTrain, yTrain, xTest, yTest = cifar()\n",
    "\n",
    "\n",
    "#build and initialize weights\n",
    "def init_weights(shape, name, glorot=False):\n",
    "    [n_inputs, n_outputs] = shape\n",
    "    init_range = sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    if glorot:\n",
    "        return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "    else:\n",
    "        return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "def momentum(list_of_weights, list_of_gradients, beta, delta):\n",
    "    # INPUT:\n",
    "    # beta is what percent of accumulated gradient you want to keep: like 0.85\n",
    "    # delta is your learning rate: like 0.01\n",
    "    \n",
    "    # OUTPUT:\n",
    "    # outputs the updaters for an accumulated step for layer's weights and the weights themselves. \n",
    "    list_of_steps = [tf.Variable(tf.zeros(weight.get_shape(), dtype=tf.float32), name='step') for weight\n",
    "                     in list_of_weights]\n",
    "    udStep = [step.assign(beta * step - (1 - beta) * delta * grad) for step, grad in \n",
    "                 zip(list_of_steps, list_of_gradients)]\n",
    "    udWt = [weight.assign(weight + step) for weight, step in zip(list_of_weights, udStep)]\n",
    "    return udStep + udWt\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 3072])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    lr = tf.constant(0.000001, dtype=tf.float32, name='lr') # if gradients approach infinity, decrease lr\n",
    "    w1 = init_weights([3072, 300], 'w1') # weights between 1st and 2nd layer\n",
    "    w2 = init_weights([300, 50], 'w2') # weights between 2nd and 3rd layer\n",
    "    w3 = init_weights([50, 10], 'w3') # weights between 3rd and 4th layer\n",
    "\n",
    "    #define network\n",
    "    percent_keep = tf.placeholder(tf.float32)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1))\n",
    "    h1 = tf.nn.dropout(h1, percent_keep) # add this line\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
    "    h2 = tf.nn.dropout(h2, percent_keep) # add this line    \n",
    "    logits = tf.matmul(h2, w3)\n",
    "    py_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(py_x, dimension=1) # actual prediction, gives the index number that has highest value\n",
    "\n",
    "    #define cost\n",
    "    rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='rows_of_cost')\n",
    "    cost = tf.reduce_mean(rows_of_cost, reduction_indices=None, keep_dims=False, \n",
    "                          name='cost')\n",
    "\n",
    "    #start building list that you'll reference in sess.run; it will execute cost function, weight updates\n",
    "    udList = [cost]\n",
    "\n",
    "    #use hand-crafted updater\n",
    "    W = [w1, w2, w3]\n",
    "    #calculate gradients\n",
    "    grad = tf.gradients(cost, W) # gradient for both weight matricies\n",
    "\n",
    "    udList = udList + momentum(W, grad, 0.85, 0.001)\n",
    "\n",
    "    #form a list of the updates - including this in sess.run will force calculation of new weights each step\n",
    "    #udList = udList + [w.assign(w - lr * g) for (w, g) in zip(W, grad)]\n",
    "\n",
    "    #use tf.optimizer by uncommenting the following two lines (and modifying where necessary)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    #train = optimizer.minimize(cost)\n",
    "\n",
    "    #output for tensorboard\n",
    "    summary1 = tf.scalar_summary(\"Cost over time\", cost) \n",
    "    summary2 = tf.histogram_summary('Weight w1 over time', w1)\n",
    "    summary3 = tf.histogram_summary('Weight w2 over time', w2)\n",
    "    summary4 = tf.histogram_summary('Weight w3 over time', w3)\n",
    "    merged = tf.merge_summary([summary1, summary2, summary3, summary4]) \n",
    "\n",
    "    #add tensorboard output to sess.run list\n",
    "    udList.append(merged)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    result = sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    miniBatchSize = 40\n",
    "    startEnd = zip(range(0, len(xTest), miniBatchSize), range(miniBatchSize, len(xTest) + 1, miniBatchSize))\n",
    "    costList = []\n",
    "    nPasses = 31\n",
    "    iteration = 0\n",
    "    for iPass in range(nPasses):\n",
    "        for (s, e) in startEnd:\n",
    "            [costVal, step1, step2, step3, weight1, weight2, weight3, tbSummary] = sess.run(\n",
    "                udList, feed_dict={X: xTest[s:e,], Y: yTest[s:e], percent_keep: 0.50})\n",
    "            writer.add_summary(tbSummary, iteration)\n",
    "            iteration += 1\n",
    "            costList.append(costVal)\n",
    "        if iPass % 5 == 0: \n",
    "            testResult = sess.run([y_pred], feed_dict={X:xTrain, percent_keep: 1.0})\n",
    "            print iPass, np.mean(np.argmax(yTrain, axis=1) == testResult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
