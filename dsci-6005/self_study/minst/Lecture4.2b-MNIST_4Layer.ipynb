{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - The 4-layer network\n",
    "\n",
    "## What you'll learn today\n",
    "1.  Why is a 4 layer network materially harder than a 3 layer network\n",
    "2.  How to overcome the training difficulties of a 4 layer network\n",
    "3.  The basic architecture and features of a modern deep learning neural net.\n",
    "4.  How to apply a deep learning network to a new problem\n",
    "5.  Some of the alterations you can make to improve performance of your deep network for a particular problem\n",
    "\n",
    "## Order of Topics\n",
    "1.  Review second Glorot, Bengio paper \n",
    "2.  Look at a 4 layer network incorporating Glorot, Bengio rectifying linear units (Relu).  \n",
    "3.  In class exercises to modify network looking for performance improvements.\n",
    "\n",
    "\n",
    "\n",
    "## Pre Reading\n",
    "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf - Glorot, Bengio paper on training  \n",
    "http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf - Glorot, Bengio - Relu in deep learning  \n",
    "http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf - Srivastava, Hinton paper on using dropout for regularization.  \n",
    "https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#dropout - Droput function in TensorFlow  \n",
    "http://climin.readthedocs.org/en/latest/rmsprop.html#id1 - look at definition of RMSProp  \n",
    "http://arxiv.org/pdf/1502.03167v3.pdf - look at batch normalization  \n",
    "\n",
    "## Modern Deep Neural Net\n",
    "\n",
    "There are several key features of this network that make it perform as well as it does.  The main features are:\n",
    "\n",
    "1.  More than one hidden layer - 4 or more network layers\n",
    "2.  Rectifying linear units for activation functions\n",
    "3.  Dropout for regularization\n",
    "4.  Use of better weight update than plain gradient descent.\n",
    "5.  Proper weight initialization.  \n",
    "\n",
    "Having more than one hidden layer defines deep learning and the Glorot Bengio paper that you saw a few lectures ago showed how the layers further from the input layer tended to saturate and stall training.  The rest of the items in the list describe the improvements that are necessary to achieve reliable training in networks with more than one hidden layer.  \n",
    "\n",
    "A rectifying linear unit is an activation function f(x) = max(0, x).  It is flat and equal to zero for negative values of its input and equal to the input for positive input values.  \n",
    "\n",
    "## Q's\n",
    "Can you think of any possible issues with the rectifying linear unit?\n",
    "\n",
    "The Glorot paper http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf explains the use and benefits of rectifying linear units.  \n",
    "\n",
    "Dropout is a method for preventing over-training in a neural net.  The basic idea is to randomly set some of the outputs from input neurons and hidden neurons to zero.  The added unreliability of inputs being present reduces the networks ability to depend too much on individual inputs and to thereby overtrain.  The basic method is outlined and discussed in the Shrivastava, Hinton paper.  http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "The code below implements a four layer fully connected network for classifying MNIST digits.  This code uses RMSProp, which is another accelerated gradient descent method that bears some similarity to AdaDelt and AdaGrad.  Here's a link to a description of RMSProp.  http://climin.readthedocs.org/en/latest/rmsprop.html#id1\n",
    "\n",
    "## Code Walk-through - group discussion\n",
    "\n",
    "1.  Look at the function definitions to get a rough idea of what each of them do.  \n",
    "2.  Walk through the main body of the code and discuss the overall structure.   \n",
    "3.  Any comments on the weight initialization?   \n",
    "4.  Walk through the handling of the weight and gradient-handling variables.  Do you see any problems with the update?   \n",
    "5.  Group coding exercise - Fix the problem with the subroutine that returns update equations.  \n",
    "6.  Rewrite the random weight initialization to incorporate Glorot scaling.   \n",
    "7.  Re-run and discuss improvement or deterioration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1289\n",
      "5 0.1484\n",
      "10 0.1644\n",
      "15 0.1783\n",
      "20 0.1924\n",
      "25 0.2054\n"
     ]
    }
   ],
   "source": [
    "# 4-layer MNIST\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from math import sqrt\n",
    "\n",
    "#build and initialize weights\n",
    "def init_weights(shape, name, glorot=False):\n",
    "    [n_inputs, n_outputs] = shape\n",
    "    init_range = sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    if glorot: return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "    else: return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "def gdUpdate(W, G, lr): # not used anywhere\n",
    "    for (w, g) in zip(W, G):\n",
    "        w.assign(w - lr * g)\n",
    "    return W[0]\n",
    "\n",
    "\n",
    "#read in the data and run through training\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    lr = tf.constant(0.00002, dtype=tf.float32, name='lr')\n",
    "\n",
    "    w1 = init_weights([784, 625], 'w1')\n",
    "    w2 = init_weights([625, 300], 'w2')\n",
    "    w3 = init_weights((300, 10), 'w3')\n",
    "\n",
    "    #define network\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1))  #look under Neural Net -> Activation in API left column\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
    "    logits = tf.matmul(h2, w3)\n",
    "    py_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(py_x, dimension=1)\n",
    "\n",
    "    #define cost\n",
    "    rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='rows_of_cost')\n",
    "    cost = tf.reduce_mean(rows_of_cost, reduction_indices=None, keep_dims=False, name='cost')\n",
    "\n",
    "    #start building list that you'll reference in sess.run\n",
    "    udList = [cost]\n",
    "\n",
    "    #use hand-crafted updater\n",
    "    W = [w1, w2, w3]\n",
    "\n",
    "    #calculate gradients\n",
    "    grad = tf.gradients(cost, W)\n",
    "\n",
    "    #form a list of the updates - including this in sess.run will force calculation of new weights each step\n",
    "    udList = udList + [w.assign(w - lr * g) for (w, g) in zip(W, grad)]\n",
    "\n",
    "    #use tf.optimizer by uncommenting the following two lines (and modifying where necessary)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    #train = optimizer.minimize(cost)\n",
    "\n",
    "    #output for tensorboard\n",
    "    summary1 = tf.scalar_summary(\"Cost over time\", cost) \n",
    "    summary2 = tf.histogram_summary('Weight w1 over time', w1)\n",
    "    summary3 = tf.histogram_summary('Weight w2 over time', w2)\n",
    "    summary4 = tf.histogram_summary('Weight w3 over time', w3)\n",
    "    merged = tf.merge_summary([summary1, summary2, summary3, summary4]) \n",
    "\n",
    "    #add tensorboard output to sess.run list\n",
    "    udList.append(merged)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    result = sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    miniBatchSize = 40\n",
    "    startEnd = zip(range(0, len(xTrain), miniBatchSize), range(miniBatchSize, len(xTrain) + 1, miniBatchSize))\n",
    "    costList = []\n",
    "    nPasses = 30\n",
    "    iteration = 0\n",
    "    for iPass in range(nPasses):\n",
    "        for (s, e) in startEnd:\n",
    "            [costVal, update1, update2, update3, tbSummary] = sess.run(udList, feed_dict={X: xTrain[s:e,], Y: yTrain[s:e]})\n",
    "            \n",
    "            writer.add_summary(tbSummary, iteration)\n",
    "            iteration += 1\n",
    "            costList.append(costVal)\n",
    "        if iPass % 5 == 0: \n",
    "            testResult = sess.run([y_pred], feed_dict={X:xTest})\n",
    "            print iPass, np.mean(np.argmax(yTest, axis=1) == testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you accidentally stop the neural net or want to continue to train the neural net without starting over:\n",
    "Replace  \n",
    "w1 = init_weights([3072, 300], 'w1')  \n",
    "w2 = init_weights([300, 10], 'w2')  \n",
    "with  \n",
    "w1 = tf.Variable(update1)  \n",
    "w2 = tf.Variable(update2)  \n",
    "where update1 and update2 are the outputs from sess.run for the weight matrices. You can save update1 and update2 in a .pkl file and start training wherever you left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q's\n",
    "This network performs at about the same level as the 3-layer network that you saw in the last lecture.  Could the problem be overfitting?  What can you look at to determine if that's true?  What further tests can you run to determine if the problem is overfitting?  What can you do to improve the performance of this classifier?  \n",
    "\n",
    "## In-class coding exercises\n",
    "Replace plain vanilla gradient descent with a different accelerated gradient method - momentum, AdaDelta, NAG or AdaGrad. Momentum is the easiest.   \n",
    "Hint: if you need the shape of a tensorflow varible, you can use tensorflow_variable.get_shape().  \n",
    "<br>\n",
    "Add dropout for regularization - What meta parameters does dropout add?  \n",
    "Hint: just add 1 placeholder for percent of neurons you want to keep and add 1 line using tf.nn.dropout(layer_here, percent_you_want_to_keep). In session.run, fill in the percent of kept neurons in feed_dict. You do this for the training and testing. In testing, you want to keep all the neurons (i.e. 1.0).    \n",
    "<br>\n",
    "Also try L2 weight penalty instead of dropout for regularization.  What meta parameters does weight penalty add?  \n",
    "Hint: just add 1 placeholder for regularization factor and modify cost function. In session.run, fill in the regularization placeholder in the feed_dict.  \n",
    "\n",
    "## Batch Normalization\n",
    "http://arxiv.org/pdf/1502.03167v3.pdf  \n",
    "https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#batch_norm  \n",
    "\n",
    "Insert the batch normalization layers (below) and compare the performance to the above neural net that doesn't include batch normalization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4-layer MNIST--completely copied from above. Please add batch norm layers. See link for code.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from math import sqrt\n",
    "\n",
    "#build and initialize weights\n",
    "def init_weights(shape, name, glorot=False):\n",
    "    [n_inputs, n_outputs] = shape\n",
    "    init_range = sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    if glorot: return tf.Variable(tf.random_uniform(shape, -init_range, init_range), name=name)\n",
    "    else: return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "    \n",
    "def gdUpdate(W, G, lr): # not used anywhere\n",
    "    for (w, g) in zip(W, G):\n",
    "        w.assign(w - lr * g)\n",
    "    return W[0]\n",
    "\n",
    "\n",
    "#read in the data and run through training\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    lr = tf.constant(0.00002, dtype=tf.float32, name='lr')\n",
    "\n",
    "    w1 = init_weights([784, 625], 'w1')\n",
    "    w2 = init_weights([625, 300], 'w2')\n",
    "    w3 = init_weights((300, 10), 'w3')\n",
    "\n",
    "    #define network\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1))  #look under Neural Net -> Activation in API left column\n",
    "    \n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2))\n",
    "    logits = tf.matmul(h2, w3)\n",
    "    py_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(py_x, dimension=1)\n",
    "\n",
    "    #define cost\n",
    "    rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='rows_of_cost')\n",
    "    cost = tf.reduce_mean(rows_of_cost, reduction_indices=None, keep_dims=False, name='cost')\n",
    "\n",
    "    #start building list that you'll reference in sess.run\n",
    "    udList = [cost]\n",
    "\n",
    "    #use hand-crafted updater\n",
    "    W = [w1, w2, w3]\n",
    "\n",
    "    #calculate gradients\n",
    "    grad = tf.gradients(cost, W)\n",
    "\n",
    "    #form a list of the updates - including this in sess.run will force calculation of new weights each step\n",
    "    udList = udList + [w.assign(w - lr * g) for (w, g) in zip(W, grad)]\n",
    "\n",
    "    #use tf.optimizer by uncommenting the following two lines (and modifying where necessary)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    #train = optimizer.minimize(cost)\n",
    "\n",
    "    #output for tensorboard\n",
    "    summary1 = tf.scalar_summary(\"Cost over time\", cost) \n",
    "    summary2 = tf.histogram_summary('Weight w1 over time', w1)\n",
    "    summary3 = tf.histogram_summary('Weight w2 over time', w2)\n",
    "    summary4 = tf.histogram_summary('Weight w3 over time', w3)\n",
    "    merged = tf.merge_summary([summary1, summary2, summary3, summary4]) \n",
    "\n",
    "    #add tensorboard output to sess.run list\n",
    "    udList.append(merged)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    result = sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter('logs/',graph=sess.graph)\n",
    "    miniBatchSize = 40\n",
    "    startEnd = zip(range(0, len(xTrain), miniBatchSize), range(miniBatchSize, len(xTrain) + 1, miniBatchSize))\n",
    "    costList = []\n",
    "    nPasses = 30\n",
    "    iteration = 0\n",
    "    for iPass in range(nPasses):\n",
    "        for (s, e) in startEnd:\n",
    "            [costVal, update1, update2, update3, tbSummary] = sess.run(udList, feed_dict={X: xTrain[s:e,], Y: yTrain[s:e]})\n",
    "            \n",
    "            writer.add_summary(tbSummary, iteration)\n",
    "            iteration += 1\n",
    "            costList.append(costVal)\n",
    "        if iPass % 5 == 0: \n",
    "            testResult = sess.run([y_pred], feed_dict={X:xTest})\n",
    "            print iPass, np.mean(np.argmax(yTest, axis=1) == testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Exercise\n",
    "Build 4-layer network for classifying Cifar images.  Use 10k training data (as in last lecture) to truncate the training time.  \n",
    "Hint: You can either copy code from last lecture's homework or from this lecture's code above. There's lots of things to change. Good luck!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
