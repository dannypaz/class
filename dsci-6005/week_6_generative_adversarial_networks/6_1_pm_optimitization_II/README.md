Optimization II
----

__Introductory__:

- Review Optimization I

__Required__:

- [Deep Learning Lecture 6: Optimization Nando de Freitas ](https://www.youtube.com/watch?v=0qUAb94CpOw) starting at 52:44
    + slides in readings folder
- [Introduction to Gradient Descent Algorithm (along with variants) ](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)
- [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)

__Optional__:

- [Adaptive Learning Rate Algorithms](https://www.youtube.com/watch?v=MmIfOx6eSpw) Good info, but damn screen flicker
- rmsprop
    - [Video](https://www.coursera.org/learn/neural-networks/lecture/YQHki/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude)
    - [Overview of mini-batch gradient descent](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) These slides are constantly referenced. You should be aware of them.

__Challenge__:

- [An Interactive Tutorial on Numerical Optimization](http://www.benfrederickson.com/numerical-optimization/)
- [Adam paper](https://arxiv.org/abs/1412.6980v8)
- [Adagrad paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)