{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimization I\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/things.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define optimization.\n",
    "- List which parts of optimization apply and do __not__ apply to Deep Learning.\n",
    "- Define gradient descent, Stochastic Gradient Descent (SGD), and momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is optimization?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finding the \"optimal\" value for something!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More technically, maximizing (or minimizing) a function by systematically choosing input values from within an allowed set and computing the value of the function\n",
    "\n",
    "A branch of Applied Mathematics used in many domains.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Mathematical_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why ML != optimization?\n",
    "-----\n",
    "\n",
    "In ML, we care about generalization. However, we lack a direct measure of generalize. Our best proxy is minimize error on training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, we need to be careful about how and which type of optimization to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/buddah.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Draw a function that has both both a local minimum and global minimum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"http://borisv.lk.net/matsc597c-1997/introduction/Lecture5/img44.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Critical points\n",
    "------\n",
    "\n",
    "One of the following:\n",
    "\n",
    "- Local maximum\n",
    "- Local minimum\n",
    "- Global maximum\n",
    "- Global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a saddle point?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://i.stack.imgur.com/cVa3m.gif\" height=\"500\"/></center>\n",
    "\n",
    "The intuition with the saddle point for a minima located close to the global minima, all directions should be climbing upward, aka going further downward is not possible. \n",
    "\n",
    "[Learn more here](http://www.offconvex.org/2016/03/22/saddlepoints/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimization is NOT critical for DL-style ML problems(in particular)\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Recovering the global minimum becomes harder as the network size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Usually all critical points are very good, so local minima are \"good enough\". [Source](https://arxiv.org/abs/1412.0233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Additionally, global minimum are irrelevant as global minimum often leads to overfitting in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is gradient descent (GD)?\n",
    "-----\n",
    "\n",
    "Gradient descent is a way to minimize an objective function J(θ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By updating the parameters in the __opposite__ direction of the gradient of the objective function ∇θJ(θ) w.r.t. to the parameters. \n",
    "\n",
    "The learning rate η (eta) determines the size of the steps we take to reach a (local) minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How is gradient descent done?\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://www.20sim.com/webhelp/steepestdescent.gif\" height=\"500\"/></center>\n",
    "\n",
    "Randomly initialize the weights of our model so we find ourselves somewhere on the horizontal plane. \n",
    "\n",
    "By evaluating the gradient at our current position, we can find the direction of steepest descent and we can take a step in that direction. \n",
    "\n",
    "Then we’ll find ourselves at a new position that’s closer to the minimum than we were before. \n",
    "\n",
    "We can re-evaluate the direction of steepest descent by taking the gradient at this new position and taking a step in this new direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/c.jpg\" width=\"300\"/></center>\n",
    "\n",
    "If you were going to explain gradient descent to your niece/nephew, what is the best metaphor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://qph.ec.quoracdn.net/main-qimg-b5ca08395cf348109c01a7364b66f2b8-p\" height=\"500\"/></center>\n",
    "\n",
    "Image you are playing video game with prizes in the valleys. The bigger the valley, the bigger the prize. We need to drive around and find the best prizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*ZmzSnV6xluGa42wtU7KYVA.gif\" height=\"500\"/></center>\n",
    "\n",
    "[Source](https://hackernoon.com/life-is-gradient-descent-880c60ac1be8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/demo.png\" height=\"500\"/></center>\n",
    "\n",
    "[Sweet demo](http://www.onmyphd.com/?p=gradient.descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2 dimensions (like WOAH DUDE!!)\n",
    "-----\n",
    "<center><img src=\"images/error.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://hduongtrong.github.io/assets/gradient_descent/gradient_descent.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is the technical definition for gradient in an error surface?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/vector.png\" width=\"500\"/></center>\n",
    "\n",
    "A vector perpendicular to the contours in the direction of the steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does gradient descent work for simple linear regression?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image%20[14].png\" height=\"500\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/gd1.png\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/gd2.png\" height=\"500\"/></center>\n",
    "\n",
    "[Sweet Demo with Python code](http://tillbergmann.com/blog/python-gradient-descent.html)  \n",
    "[Learn more here](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When would you use gradient descent to optimize linear regression?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Too many rows (observations) or columns (features) that would make the inverse required for the Normal Equation __not__ feasible!\n",
    "\n",
    "<center><img src=\"https://essentialproject.files.wordpress.com/2013/12/normal-equation.jpg\" height=\"500\"/></center>\n",
    "\n",
    "[Learn more here](https://www.quora.com/Convex-Optimization-Why-use-gradient-descent-when-the-normal-equation-exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grad Student Descent (GSD)\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://s-media-cache-ak0.pinimg.com/originals/68/4d/b6/684db67fd115019c7f62ddd9b591b2f3.gif\" height=\"500\"/></center>\n",
    "\n",
    "> A graduate student fiddles around with the parameters until it works. \n",
    "\n",
    "> It’s kind of sad, but works surprisingly well!\n",
    "\n",
    "[Source](https://sciencedryad.wordpress.com/2014/01/25/grad-student-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why is gradient descent a challenge for DL?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Data size / Parameter size\n",
    "2. Local information inference for global structure\n",
    "3. Nonconvex optimization function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Data size\n",
    "------\n",
    "\n",
    "<center><img src=\"https://cdn.meme.am/cache/instances/folder569/65051569.jpg\" height=\"500\"/></center>\n",
    "\n",
    "If our data size is large (and our model is deep), it becomes prohibitive to over all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Local information inference for global structure\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/fields.png\" height=\"500\"/></center>\n",
    "\n",
    "Forced to use local information to infer the global structure of the error surface.\n",
    "\n",
    "We update the weights based on a single training example with \"local receptive fields\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DL is nonconvex (maybe)\n",
    "------\n",
    "\n",
    "There may or may not be \"many\" local minima. \n",
    "\n",
    "The current thinking is there an abundance of saddle points. We can escape saddle points!\n",
    "\n",
    "[Learn more here](http://www.offconvex.org/2016/01/25/non-convex-workshop/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead of using math, we must on a number of heuristics (e.g., number of epochs, learning rate, momentum, batch size, …)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stochastic Gradient Descent (SGD)\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://sebastianruder.com/content/images/2016/09/sgd_fluctuation.png\" height=\"500\"/></center>\n",
    "\n",
    "Performs a parameter update for __each__ training example.\n",
    "\n",
    "[Demo code](https://github.com/dtnewman/gradient_descent/blob/master/stochastic_gradient_descent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mini-batch gradient descent\n",
    "------\n",
    "\n",
    "__What?__  \n",
    "\n",
    "Performs an update for every mini-batch of nn training examples. Middle ground between \"true\" (n=1) stochastic gradient and full (n=N) gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Why__?  \n",
    "- Faster than gradient descent \n",
    "- More stable (reduce variance) than SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the training difference between gradient descent and SGD?\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/stochastic-vs-batch-gradient-descent.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/vs.png\" height=\"500\"/></center>\n",
    "\n",
    "[Source](https://plot.ly/create/?fid=brianjspiering:84&fid=michaeljancsy:447)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SGD problem\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/wom.png\" width=\"500\"/></center> \n",
    "\n",
    "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another. \n",
    "\n",
    "In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.\n",
    "\n",
    "[Source](https://www.slideshare.net/cfregly/gradient-descent-back-propagation-and-auto-differentiation-advanced-spark-and-tensorflow-meetup-08042016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Momentum as a solution\n",
    "----\n",
    "\n",
    "Momentum helps for faster convergence in SGD in the relevant direction and dampens oscillations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Momentum is an exponentially weighted moving average\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/wm.png\" width=\"500\"/></center>\n",
    "\n",
    "Momentum smooths volatility in the step sizes during a random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/mom.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Momentum Problem\n",
    "-----\n",
    "\n",
    "The\tstandard momentum\tmethod\tfirst\tcomputes\tthe\tgradient\tat\tthe\tcurrent\tlocation\tand\tthen\ttakes\ta big\tjump\tin\tthe\tdirecti on\tof\tthe\tupdated\taccumulated\tgradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nesterov Accelerated Gradient (NAG) as a solution\n",
    "------\n",
    "\n",
    "If using the past information helps, let's use future information (seriously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before updating our weights, look ahead to where we have accumulated momentum.\n",
    "\n",
    "Adjust our update based on the “future”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://cs231n.github.io/assets/nn3/nesterov.jpeg\" width=\"1000\"/></center>\n",
    "\n",
    "Nesterov method takes the \"gamble -> correction\" approach.\n",
    "\n",
    "[Source](http://cs231n.github.io/neural-networks-3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/nes.png\" width=\"750\"/></center>\n",
    "\n",
    "1. Make\ta big jump (brown) in\tthe\tdirection of the previous accumulated gradient.\t\n",
    "2. Then\tmeasure\tthe\tgradient where\tyou\tend\tup\tand\tmake a corretion (red).\n",
    "\n",
    "[Source](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "If we see multiple gradients in a row with same direction, momentum means we should: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "take a larger step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decay (The last trick of the day)\n",
    "-----\n",
    "\n",
    "Helpful to anneal the learning rate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Anneal\n",
    "----\n",
    "\n",
    "<center><img src=\"https://i.ytimg.com/vi/uUw8XDhX8zY/maxresdefault.jpg\" width=\"500\"/></center>\n",
    "\n",
    "> Heat (metal or glass) and allow it to cool slowly, in order to remove internal stresses and toughen it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Proper Learning Rate decay can be tricky\n",
    "----\n",
    "\n",
    "Decay it slowly and you’ll be wasting computation bouncing around chaotically with little improvement for a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But decay it too aggressively and the system will cool too quickly, unable to reach the best position it can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://cs231n.github.io/assets/nn3/opt2.gif\" height=\"500\"/></center>\n",
    "\n",
    "Notice that SGD has a very hard time breaking symmetry and gets stuck on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keras' SGD\n",
    "------\n",
    "<br>\n",
    "```\n",
    "Includes support for momentum, learning rate decay, and Nesterov momentum.\n",
    "\n",
    "Arguments\n",
    "\n",
    "lr: float >= 0. Learning rate.\n",
    "momentum: float >= 0. Parameter updates momentum.\n",
    "decay: float >= 0. Learning rate decay over each update.\n",
    "nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://cs231n.github.io/assets/nn3/opt1.gif\" height=\"500\"/></center>\n",
    "\n",
    "Our IRL friends (SGD, Mometum, and NAG) can get stuck in saddle points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Keras Optimizers](https://keras.io/optimizers/)\n",
    "-----\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c9/Keras_Logo.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- RMSprop\n",
    "- Adagrad\n",
    "- Adadelta\n",
    "- Adam\n",
    "- Adamax\n",
    "- Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://img.memesuper.com/3cadb615f94bbca4851070674f4c647e_thats-okay-maybe-next-time-maybe-next-time-meme_625-416.jpeg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Keras only has 1st order optimizers. What are 2nd order optimizers?\n",
    "\n",
    "Why are __no__ 2nd order optimizers included in Keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2nd order optimizers often require the exact evaluation of the Hessian, a square matrix of second-order partial derivatives of a scalar-valued function. \n",
    "\n",
    "Thus obtain a richer information about the curvature of a parameter space, and hence speed up the convergence of the optimization method. \n",
    "\n",
    "That is quite expensive in computational terms. The number of parameters in DL makes it intractable for 2nd-order optimization methods such as Newton's method.\n",
    "\n",
    "[Source](http://sebastianruder.com/optimizing-gradient-descent/)\n",
    "\n",
    "[However, you can learn more 2nd order optimizers](https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network)\n",
    "\n",
    "[Another very rad intro to advanced optimitization](http://www.benfrederickson.com/numerical-optimization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "Summary\n",
    "----\n",
    "\n",
    "- Optimization is powerful method to find solutions to problems. However, do not be seduced by the \"dark\" / math side.\n",
    "- DL is too complex and does __not__ really need \"full\" optimization.\n",
    "- We are going to use mini-batch gradient descent. Typically called Stochastic Gradient Descent (SGD).\n",
    "- We'll speed it up by adding momentum. If we are going downhill, GO FASTER!\n",
    "- We'll slow it down over time with decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conditioning\n",
    "------\n",
    "\n",
    "<center><img src=\"images/ill.png\" height=\"500\"/></center>\n",
    "\n",
    "Conditioning refers to how rapidly a function changes with respect to small changes in its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ill conditioned\n",
    "-----\n",
    "\n",
    "The gradient changes __rapidly__ underneath our feet as we move in the direction of steepest descent, aka the gradient is wobbly.\n",
    "\n",
    "Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation because rounding errors in the inputs can result in large changes in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2nd-order methods\n",
    "-----\n",
    "\n",
    "1. Newton’s method\n",
    "2. Quasi-Newton, Gauss-Newton\n",
    "3. BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
