RAT: Optimitization I
----
Individual Name: ___________________  
Team Names: _____________ &nbsp;&nbsp;&nbsp; _____________ &nbsp;&nbsp;&nbsp; _____________ &nbsp;&nbsp; _____________ &nbsp;&nbsp;&nbsp; _____________

1)  What is the difference between batch gradient descent and (mini-batch) stochastic gradient descent (SGD)?

<br>
<br>
<br>

2) DL primarily uses SGD (and variations). What is SGD's biggest limitation when used on other machine learning problems?

<br>
<br>
<br>

3) Explain  how increasing the number of dimensions can turn local minima into saddle points.

<br>
<br>
<br>
<br>
<br>

-----
Challenge Question
------

1) Why is the mini-batch default size 128? (andâ€¦ Why is that important?)

<br>

2) Explain how Nesterov Accelerated Gradient (NAG) "accelerates" over standard momentum.

<br>
<br>
<br>

3) Write pseudo code one-liner for NAG: