{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training Neural Networks\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://cdn.meme.am/instances/500x/65051569/waiting-skeleton-near-computer-still-waiting-for-my-neural-network.jpg\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- List the hyperparameters to tune for a NN\n",
    "- Design the best general architecture for a NN\n",
    "- Compare activation functions\n",
    "- Explain how learning rate effects model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is goal of machine learning?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Automatically learning from data to generalize (across domain and time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hyperparameters to tune\n",
    "-----\n",
    "\n",
    "1. Number of epochs\n",
    "1. Architecture \n",
    "1. Activation function\n",
    "1. Weight update rate, aka learning rate\n",
    "1. Define Training set\n",
    "1. Weight update algorithm, aka optimitizer \n",
    "1. Regularization\n",
    "1. Minibatch size\n",
    "1. Batch normalization \n",
    "1. Types of optimizers\n",
    "1. Data Augmentation\n",
    "1. Weight sharing\n",
    "1. Fine tuning, aka pretrained networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are epochs?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A single iteration over the entire training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a trade-off between training time and test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Architecture \n",
    "------\n",
    "\n",
    "1. Number of nodes in each layer \n",
    "1. Number of layers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Number of nodes in each layer\n",
    "-------\n",
    "\n",
    "Why do we want to stack NN layers (instead of very wide layers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Better at learning combinations of features!\n",
    "\n",
    "Very wide, shallow networks are very good at memorization, but not so good at generalization. If you train the network with every possible input value, a super wide network could eventually memorize the corresponding output value that you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Add layers: Always go deeper!\n",
    "------\n",
    "\n",
    "<center><img src=\"https://cdn-business.discourse.org/uploads/analyticsvidhya/original/2X/5/55cce711aecac89c48d50691cf1c525f785d794f.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll more tricks later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Activation Function\n",
    "------\n",
    "\n",
    "To allow Neural Networks to learn complex decision boundaries, \n",
    "we apply a nonlinear activation function to some of its layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "What is the fundamental requirement for an activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network function must be differentiable because of back prop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Commonly used activation functions:\n",
    "-------\n",
    "\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is limiting about sigmoid?\n",
    "-------\n",
    "\n",
    "1. A node's activation saturates at either tail of 0 or 1, \n",
    "2. Local gradient maximium at .25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sigmoids saturate and kill gradients.\n",
    "-------\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*gkXI7LYwyGPLU5dn6Jb6Bg.png\" height=\"500\"/></center>\n",
    "\n",
    "A node's activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. \n",
    "\n",
    "[Source](http://www.kdnuggets.com/2016/03/must-know-tips-deep-learning-part-2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Local gradient maximum at 0.25,\n",
    "------\n",
    "<center><img src=\"https://qph.ec.quoracdn.net/main-qimg-b89e3c9b324b958b1b38ec2976a18583\" height=\"500\"/></center>\n",
    "\n",
    "Another non-obvious fun fact about sigmoid is that its local gradient achieves a maximum at 0.25, when z = 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.\n",
    "\n",
    "[Source](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/relu.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the derivate of relu?\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*g0yxlK8kEBw8uA1f82XQdA.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the advantages of ReLU?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1) ReLUs does not suffer from saturating. It go to ∞!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Induces the sparsity in the hidden units. Hidden units can be driven to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Deeper networks can be trained because it minimizes the \"gradient vanishing problem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/more_relu.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a drawback of ReLU?\n",
    "-----\n",
    "\n",
    "__“Dead ReLU” problem__\n",
    "\n",
    "If a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain __permanently dead__.\n",
    "\n",
    "[Source 1](http://www.kdnuggets.com/2016/03/must-know-tips-deep-learning-part-2.html)  \n",
    "[Source 2](http://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Weight update rate, aka learning rate\n",
    "------\n",
    "\n",
    "Picking the optimal learning rate is a impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Picking a mediocre learning rate is a easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Picking a decent learning rate is the goal (and an art)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/learning_rate.png\" height=\"500\"/></center>\n",
    "\n",
    "[Source](http://cs231n.github.io/neural-networks-3/#baby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we pick a learning rate that's too big, we'll mostly likely start diverging (weights will go to infinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we pick a learning rate that's too small, we risk taking too long during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do I know when to stop?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When there are no more errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Handling Errors\n",
    "------\n",
    "\n",
    "<center><img src=\"images/errors.jpg\" height=\"500\"/></center>\n",
    "\n",
    "[Source](http://bytes.schibsted.com/deep-learning-changing-data-science-paradigms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The power of common objective benchmarks\n",
    "-----\n",
    "\n",
    "Machine learning's secret sauce.\n",
    "\n",
    "Everyone competes on single dataset with clear benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Other disciplines do not use it, for example Computer Science and Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How good is \"good enough\" on MNIST?\n",
    "------\n",
    "\n",
    "[ML benchmarks](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lab tips:\n",
    "------\n",
    "\n",
    "> Being a machine learning researcher is a lot like being an addict at a slot machine, forever running experiments to see if intuitions about hyperparameters or setups are working,\n",
    "\n",
    "> This sort of slot machine mentality does not encourage good science. Perhaps by chance we get to a set of parameters that “looks promising”.\n",
    "\n",
    "[Source](https://jack-clark.net/2017/03/27/import-ai-issue-35-the-end-of-imagenet-unsupervised-image-fiddling-with-discogan-and-alibabas-voice-data-stockpile/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lab tips: Take detailed notes!\n",
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"images/time.png\" height=\"500\"/></center>\n",
    "\n",
    "[Source](https://www.slideshare.net/JenAman/large-scale-deep-learning-with-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- There __a lot__ of NN hyperparameters. Choose wisely.\n",
    "- Architectures should be deep and simple.\n",
    "- ReLU is currently the best activation function.\n",
    "- It is an art to find the \"Goldilocks\" learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://7pn4yt.com1.z0.glb.clouddn.com/blog-prelu.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
