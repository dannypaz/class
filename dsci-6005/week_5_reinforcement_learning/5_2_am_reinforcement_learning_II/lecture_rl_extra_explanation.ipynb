{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://s-media-cache-ak0.pinimg.com/originals/65/4c/eb/654ceb3d7674222b6b44fc7b7f5e5331.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov decision processes (MDPs)\n",
    "-----\n",
    "\n",
    "provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov decision processes (MDPs)\n",
    "-----\n",
    "\n",
    "At each time step, the process is in some state s, and the decision maker may choose any action a that is available in state s. \n",
    "\n",
    "The process responds at the next time step by randomly moving into a new state s', and giving the agent a corresponding reward R<sub>a</sub>(s ,s')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The probability that the process moves into its new state s' is influenced by the chosen action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Specifically, it is given by the state transition function P<sub>a</sub>(s, s').\n",
    "\n",
    "Thus, the next state s' depends on the current state s and the agent's action a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MDP connection to Reinforcement Learning (RL)\n",
    "-------\n",
    "\n",
    "If the MDP probabilities or rewards are unknown at the begining, then Reinforcement Learning FTW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "MDP probabilities or rewards can be unknown at the begining of learning. Can they be unknownable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__No.__\n",
    "\n",
    "The agents must receive rewards and probabilities can be estimated.\n",
    "\n",
    "MDP probabilities or rewards can be deterministic or stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-learning\n",
    "------\n",
    "\n",
    "For this purpose it is useful to define a further function, which corresponds to taking the action  a and then continuing optimally (or according to whatever policy one currently has):\n",
    "\n",
    "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/63b502aafbe6ea1585231222ea3783f40f0808a9\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While this function is also unknown, experience during learning is based on (s, a) pairs (together with the outcome s'; that is, \"I was in state s and I tried doing a and s' happened\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, this is an array Q and uses experience to update it directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reinforcement learning like magic ðŸŽ©\n",
    "-----\n",
    "\n",
    "RL can solve Markov decision processes without explicit specification of the transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like the Normal Distribution Assumptionâ€¦\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://www.nate-miller.org/uploads/1/3/4/0/13405886/4526333.jpg?364\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-Learning Example\n",
    "------\n",
    "\n",
    "<center><img src=\"http://blog-assets.bigfishgames.com/uploads/2012/08/Choose-your-own-adventure.jpg\" height=\"500\"/></center>\n",
    "\n",
    "Who wants to play?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/image/Modeling-Environment_clip_image002.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/image/Modeling-Environment_clip_image004.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/image/Modeling-Environment_clip_image006.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
