{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Keras neural network to play Catch with Reinforcement Learning\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: For the game of _Catch_, what is the:\n",
    "\n",
    "- environment\n",
    "- agent\n",
    "- actions\n",
    "- reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__:\n",
    "\n",
    "- environment: The grid size of \"pixels\" \n",
    "- agent: the basket\n",
    "- actions: \"Left\", \"Stay\", \"Right\",\n",
    "- reward: the fruit in the basket at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Where in the code is the Reinforcement Learning? How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions:__\n",
    "\n",
    "in the Catch class, it uses the given parameters to make the updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Reinforcement Learning has a notion of exploration. Where in the code is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__:\n",
    "\n",
    "```python\n",
    "while not game_over:\n",
    "    input_tm1 = input_t\n",
    "    # get next action\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = np.random.randint(0, num_actions, size=1)\n",
    "```\n",
    "\n",
    "https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L147-L148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Does a greedy algorithm work for the game of Catch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__: Yes. The networks see the entire 10x10 pixels grid as input and outputs three values, each value corresponds to an action (move left, stay, move right). Since these values represent the expected accumulated future reward,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Summarize this model in a couple of sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__: A very simple MLP that takes in the environment, weights it, picks the best of the 3 actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: How does this Keras NN connect to the Reinforcement Learning reward? For example, what is the training signal for backprop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__: It waits until a \"batch\" of plays, stored in exp_replay, then uses win count as training signal for backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Train model to get max points in test. You can tune current model or define another architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
