{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reinforcement Learning Algorithms\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11131906/figtmp7.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Describe explore / exploit algorithms\n",
    "- Diagram Markov Decision Process (MDP)\n",
    "- Define Q-learning, a common Reinforcement Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a local minimum in RL?\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://discovery.rsm.nl/fileadmin/_processed_/3/8/csm_e91ba7f6426ff24df53fe1a29a3bbf4b-stefano_puntoni_bc63d30303.jpg\" height=\"500\"/></center>\n",
    "\n",
    "The agents takes a lower reward (short-term) action to get a larger reward (long-term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Overcoming Local Minimum: Explore vs Exploit\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/mab-2.jpg?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Fbandits%2Fmab-2.jpg\" height=\"500\"/></center>\n",
    "\n",
    "- Exploitation: Make the best decision given current information\n",
    "- Exploration: Gather more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source 1](https://www.quora.com/Is-overfitting-a-problem-in-deep-reinforcement-learning)  \n",
    "[Source 2](http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For each environment, list the Explore and Exploit:\n",
    "------\n",
    "\n",
    "Restaurant Selection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Exploitation: Go to favorite restaurant \n",
    "- Exploration: Try a new restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Game Playing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Exploitation: Play the move you believe is best \n",
    "- Exploration: Play an experimental move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are Explore / Exploit Strategy algorithms?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ε–Greedy\n",
    "- Bayesian Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ε–Greedy\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"images/ep.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian Bandits\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.chrisstucchio.com/blog/2013/bayesian_bandit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Calculate Posterior for each \"arm\" / action\n",
    "2. Pick a random sample from each Posterior\n",
    "3. Choose the arm with highest reward\n",
    "4. Update Posteriors for next round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Demo!](https://learnforeverlearn.com/bandits/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Decision Process (MDP) Formalism\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/mdp.png\" wdith=\"500\"/></center>\n",
    "\n",
    "Agent interacts with environment\n",
    "- Environment states: s∈S\n",
    "- Agent actions: a∈A\n",
    "- State transition: P(s<sub>t+1</sub>∣s<sub>t</sub>,a<sub>t</sub>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What assumption does the Station transition P(s<sub>t+1</sub>∣s<sub>t</sub>,a<sub>t</sub>) make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Markov Assumption__\n",
    "\n",
    "> Given the Present, The Future Does Not Depend On the Past "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Decision Process (MDP) Example\n",
    "----\n",
    "<center><img src=\"images/mdp_2.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MDP\n",
    "-----\n",
    "\n",
    "Given one run of the Markov decision process, we can easily calculate the total reward for one episode:\n",
    "\n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.26-AM.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given that, the total future reward from time point t onward can be expressed as:\n",
    "    \n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.32-AM.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use __discounted future reward__ instead:\n",
    "\n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.36-AM.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here γ is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step t can be expressed in terms of the same thing at time step t+1:\n",
    "\n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.40-AM.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we set the discount factor γ=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like γ=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor γ=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another RL jargon: Policy\n",
    "------\n",
    "\n",
    "<center><img src=\"images/policy.jpg\" height=\"500\"/></center>\n",
    "\n",
    "The rules for how the agent chooses among the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A good policy for an agent would be to always choose an action that maximizes the (discounted) future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Decision Process (MDP)\n",
    "-----\n",
    "\n",
    "< S, A, P, R, γ >\n",
    "\n",
    "- S is a finite set of states\n",
    "- A is a finite set of actions\n",
    "- P is a state transition probability matrix\n",
    "- R is a reward function\n",
    "- γ (gamma) is a discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Policy is the behavior chooser for an agent\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deterministic policy π(s) = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Stochastic policy: π(a|s) = P(A<sub>t</sub>=a | S<sub>t</sub> = s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-learning\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/12042140/11038f3.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-learning\n",
    "-----\n",
    "\n",
    "A __model-free__ technique to learn an optimal Q(s, a) for the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model-Free Reinforcement Learning \n",
    "-------\n",
    "\n",
    "Do __not__ learn a model of the world. \n",
    "\n",
    "Do __not__ explicitly learn transition probabilities or reward functions. \n",
    "\n",
    "Using sampling (Stats FTW!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-learning is brilliant because it only needs to know what action to take, not why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Only try to learn the Q-values of actions, or only learn the policy. \n",
    "\n",
    "Essentially, just learn the mapping from states to actions. Modeling how much we're expecting to get in the long run. The algorithm learns directly when to take what action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model-Based Reinforcement Learning\n",
    "---------\n",
    "\n",
    "Keep track of the transition probabilities and the reward function. \n",
    "\n",
    "These are typically learned as parametrized models. \n",
    "\n",
    "The models learn what the effect is going to be of taking an particular action in a particular state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.quora.com/What-is-an-intuitive-explanation-of-what-model-based-reinforcement-learning-is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://www.cs.princeton.edu/~andyz/img/proj/ai/pacmanRL/pacmanQLfeature.gif\" height=\"500\"/></center>\n",
    "\n",
    "Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It works by learning an action-value function that ultimately gives the expected utility of taking a __given action in a given state__ and following the optimal policy thereafter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Which action is optimal for each state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The action that has the highest expected long-term reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How Q-learning works\n",
    "-----\n",
    "\n",
    "Before learning has started, Q returns an (arbitrary) fixed value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, each time step:\n",
    "\n",
    "- The agent selects an action\n",
    "- Observes a reward \n",
    "- Observes a new state that may depend on both the previous state and the selected action\n",
    "- Q is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-Learning\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.47-AM.png\" height=\"500\"/></center>\n",
    "\n",
    "A function Q(s, a) representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is called Q-function, because it represents the __“quality”__ of a certain action in a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pick the action with the highest Q-value\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.09.56-AM.png\" height=\"500\"/></center>\n",
    "\n",
    "π represents the policy, the rule how we choose an action in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bellman equation\n",
    "----\n",
    "\n",
    "<center><img src=\"https://www.nervanasys.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-21-at-11.10.00-AM.png\" height=\"500\"/></center>\n",
    "\n",
    "Express the Q-value of state s and action a in terms of the Q-value of the next state s’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-Learning\n",
    "----\n",
    "\n",
    "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7a2a11876f4a2bef1198beb780a769cfa5c21af3\" height=\"500\"/></center>\n",
    "\n",
    "A simple value iteration update assuming the old value and makes a correction based on the new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q-learning Formalism\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"images/q.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "α: Learning Rate\n",
    "----\n",
    "\n",
    "The learning rate or step size determines to what extent the newly acquired information will override the old information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "0 will make the agent not learn anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1 will make the agent consider only the most recent information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "In fully deterministic environments, what should learning rate be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "α = 1 is optimal in fully deterministic environments. The most recent information is all that is needed because the future will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are limitations of Q-learning?\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finite step assumption\n",
    "- Finite number of actions\n",
    "- Finite state space\n",
    "- Extreme local minimum / delayed rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Challenge Question\n",
    "------\n",
    "\n",
    "If your state space very large and you still want to apply Q-learning, what should you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compression through feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is very good at feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://blogs-images.forbes.com/markhughes/files/2016/01/Terminator-2-1200x873.jpg?width=960\" width=\"500\"/></center>\n",
    "\n",
    "__Deep Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- ε–Greedy & Bayesian Bandits are explore/exploit algorithms\n",
    "- Markov Decision Process (MDP) is a way to define discrete, finite state transition probabilities \n",
    "- Q-learning finds an optimal action-selection policy for a MDP\n",
    "- Q-learning learns an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning\n",
    "-----\n",
    "\n",
    "A __model-free__, __off-policy__ technique to learn an optimal Q(s, a) for the agent.\n",
    "\n",
    "__Off-policy__: learning optimal policy independently of agent's actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Difference between on policy and off policy](https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bellman.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Policy Gradients\n",
    "------\n",
    "\n",
    "Work better than Q Learning, but I don't have time!\n",
    "\n",
    "[Learn more here](http://karpathy.github.io/2016/05/31/rl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "Is RL online decision-making? Why or Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes. The agent needs to make decision as events are happening with incomplete information"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
