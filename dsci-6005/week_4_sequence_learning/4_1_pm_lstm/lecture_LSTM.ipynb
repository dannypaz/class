{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Long Short-Term Memory networks (LSTM)\n",
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://s-media-cache-ak0.pinimg.com/564x/60/51/53/60515323dd5f0961816bb062b0db11fc.jpg\" width=\"410\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Describe the connection between RNN and LSTM.\n",
    "- Describe how LSTM are different from RNN\n",
    "- Diagram a gated memory cell\n",
    "- List LSTM's potential limitations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What tasks are a recurrent neural network (RNN)  useful for?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/rnn.png\" height=\"500\"/></center>\n",
    "\n",
    "Recognizing patterns in data sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does a RNN learn sequences?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/rnn_lines.png\" width=\"500\"/></center>\n",
    "\n",
    "Each RNN's input is the current input example __and__ also what the input from one timestep back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RNN modeling sequences\n",
    "------\n",
    "<center><img src=\"https://i.imgur.com/kpZBDfV.gif\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a Long Short-Term Memory (LSTM) network?\n",
    "----\n",
    "\n",
    "A RNN plus memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the mid-90s, the German researchers Sepp Hochreiter and Juergen Schmidhuber proposed LSTMs as a solution to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "LSTMs help preserve the error so it can be better backpropagated through time and layers. \n",
    "\n",
    "By maintaining a more constant error, LSTMs allow RNN-type models to continue to learn over many time steps (over ~1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the mechanism in LSTM to handle long-term dependencies?\n",
    "--------\n",
    "\n",
    "LSTMs contain information outside the normal flow of the RNN in a __gated cell__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gated Cell\n",
    "----\n",
    "\n",
    "<center><img src=\"images/gated.png\" height=\"500\"/></center>\n",
    "\n",
    "r is reset gate  \n",
    "z is update gate  \n",
    "h is the activation gate   \n",
    "~h is the candidate activation agate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gated RNNs\n",
    "-----\n",
    "\n",
    "Instead of manually deciding when to clear the state, the neural network learns to decide when to do it.\n",
    "\n",
    "Thus creating paths through time that have derivatives that neither vanish nor explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gated RNNs\n",
    "----\n",
    "\n",
    "<center><img src=\"images/gated2.png\" height=\"500\"/></center>\n",
    "\n",
    "[Source](https://www.slideshare.net/odsc/alec-radfordodsc-presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gated RNNs over time\n",
    "-----\n",
    "<center><img src=\"images/gated_time.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM has a gated cell for \"memory\"\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/ltsm.png\" height=\"500\"/></center>\n",
    "\n",
    "i is input gate  \n",
    "f is forgot gate   \n",
    "o is output gate  \n",
    "c is memory cell content  \n",
    "~c is new memory cell content  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM memory cell\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://deeplearning4j.org/img/gers_lstm.png\" width=\"500\"/></center>\n",
    "\n",
    "1. An input gate\n",
    "2. An outptut gate\n",
    "3. A keep/forget data (\"memory\") gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTMs in a network\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/chain.png\" height=\"500\"/></center>\n",
    "\n",
    "A chain structure\n",
    "\n",
    "[Source](https://ahmedhanibrahim.wordpress.com/2016/10/09/another-lstm-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Given this is the internal node structure for LSTM, what is the internal node structure for RNN?\n",
    "\n",
    "<center><img src=\"images/chain.png\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/rnn2.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is language modeling?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predict next word based on previous words.\n",
    "\n",
    "> Do you like Green eggs and ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM walkthrough\n",
    "-----\n",
    "\n",
    "Predict the gender of the subject based on previous sequence of words.\n",
    "\n",
    "> Chris is my  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/1step.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we see a new subject we want to forget old subject.\n",
    "\n",
    "> Chris is my aunt. Cameron is my ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/2step.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/3step.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/4step.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://www.cedar.buffalo.edu/~srihari/CSE676/LSTM.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are LSTM limitations?\n",
    "-------\n",
    "\n",
    "Speed is a problem for sequence-to-sequence LSTM models. These models are large, we need more computing power. \n",
    "    \n",
    "__Solution__: new hardware trading less precision for more computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are LSTM limitations?\n",
    "-------\n",
    "\n",
    "These model are sequential, can be slow even at small sizes.\n",
    "\n",
    "__Solution__: new, more parallel models (Neural GPU, [ByteNet](https://arxiv.org/abs/1610.10099))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are LSTM limitations?\n",
    "-------\n",
    "\n",
    "Sequence-to-sequence LSTMs require a lot of data.\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Attention and other new architectures increase data efficiency.\n",
    "- Use regularizers like dropout, confidence penalty and layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "\n",
    "How many gates does a LSTM node have? What is the general goal of these gates? How does each gate contribute to that goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "LSTM has 3 gates (input, output, memory) whose goal is to control the the state of the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "Summary\n",
    "----\n",
    "\n",
    "- LSTMs have a sense of memory to capture repeating patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- RNN Backpropagation through time (BPTT) but are limited in \"time\"\n",
    "- LSTM have extended time window through \"memory\"\n",
    "- Memory is implemented through a gated cell which learns which features to retain over training\n",
    "- LSTM require more data and train slowly (sequential and more parameters)\n",
    "- LSTM are currently useful but a likely to be replaced in the near future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Bonus Material\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM's use of self-loops\n",
    "-----\n",
    "\n",
    "Self-loops produce paths where the gradient can flow for long durations. \n",
    "\n",
    "Weights on this self-loop are conditioned on the context, rather than fixed. By making the weight of this self-loop gated (controlled by another hidden unit), the time scale of integration can be changed dynamically. \n",
    "\n",
    "Even for an LSTM with fixed parameters, the time scale of integration can change based on the input sequence, because the time constants are output by the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/lstm 2.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://deeplearning4j.org/img/greff_lstm_diagram.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM Variants\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/variants.png\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
