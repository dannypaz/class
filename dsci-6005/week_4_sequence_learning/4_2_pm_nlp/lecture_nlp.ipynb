{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DL for NLP\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://rutumulkar.com/public/images/blog/nlp-ml.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain how DL can improve Information Retrieval\n",
    "- Explain how DL can improve Question Answering\n",
    "- Apply CNN to NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Information Retrieval (IR)?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on a user query, search \"documents\" and present user with the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From strings to things\n",
    "-------\n",
    "\n",
    "<center><img src=\"https://www.google.com/intl/es419/insidesearch/features/search/assets/img/static-graph.png\" width=\"500\"/></center>\n",
    "\n",
    "Search intent\n",
    "\n",
    "What is the meaning, context, and relevant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RankBrain\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://montfort.io/wp-content/uploads/2015/11/Google-Rank-Brain.jpg\" height=\"500\"/></center>\n",
    "\n",
    "An \"A.I\" system that allows Google to interpret searches and display results that might __not__ have the exact words that were searched for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How RankBrain helps Google\n",
    "------\n",
    "\n",
    "- __Novel Queries__: If RankBrain sees a word or phrase it isn’t familiar with, makes a guess as to what words or phrases might have a similar meaning and search accordingly. Among the 3 billion queries that Google processes __everyday__, 15% are queries that have never been seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __Complex Searches__: Complex multi-word queries (also called “long-tail” queries). Effectively interpret and \"re-write\" those queries in order to find and display the most suitable results for the searcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How RankBrain helps Google\n",
    "------\n",
    "\n",
    "- __Ambiguous Searches__: Deliver relevant results for ambiguous searches because it can form associations and make sense of these queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __Higher Than Human Accuracy__: The Google engineers were correct about 70% of the time, while RankBrain was correct at least 80% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.quora.com/How-does-Googles-RankBrain-signal-work-What-do-we-know-about-it-so-far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Any guesses on how RankBrain does that \"trick\"?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/vectors.png\" height=\"500\"/></center>\n",
    "\n",
    "Word / Document / Thought vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just one part of Google Search\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://qph.ec.quoracdn.net/main-qimg-9045d2de6ae3a0bc02f1a65efbdc1f9d-p\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP: It's All Question Answering\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Google: What do I want?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Language Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;What is the next word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Summarization: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What is the gist of this document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP: It's All Question Answering\n",
    "------\n",
    "- Part Of Speech Tagging: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What is the POS of this token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Translation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What is this sentence in Arabic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__CHATBOTS__: \n",
    "-------\n",
    "\n",
    "<center><img src=\"https://tctechcrunch2011.files.wordpress.com/2016/05/robot-customer-service.png?w=738\" height=\"500\"/></center>\n",
    "\n",
    "Question Answering all the way down!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dynamic Coattention Network (DCN): The Promise\n",
    "-----\n",
    "\n",
    "Do __not__ produce a single, static representation of the document without context.\n",
    "\n",
    "Interpret the document differently depending on the question.\n",
    "\n",
    "Use the same corpus for many NLP tasks; In theory, construct a \"universal\" corpus for all NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dynamic Coattention Network (DCN): The Mechanics\n",
    "-----\n",
    "\n",
    "Naive approach: cram as much information about the document as possible, not knowing what the questions will be (similar to the traditional approach of building a static representation).\n",
    "\n",
    "DCN approach: Read the document again for each question, based on the question (building a conditional representation of the document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dynamic Coattention Network (DCN): Example\n",
    "-----\n",
    "\n",
    "Document:\n",
    "\n",
    "> On Carolina's next possession fullback Mike Tolbert lost a fumble while being tackled by safety Darian Stewart, which linebacker Danny Trevathan recovered on the Broncos 40-yard line. However, the Panthers soon took the ball back when defensive end Kony Ealy tipped a Manning pass to himself and then intercepted it, returning the ball 19 yards to the Panthers 39-yard line with 1:55 left on the clock. \n",
    "\n",
    "Question:\n",
    "\n",
    "> Who recovered Tolbert's fumble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dynamic Coattention Network (DCN): Example\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://metamind.io/static/images/layouts/research/squad-qa-attn.svg\" height=\"500\"/></center>\n",
    "\n",
    "The top row is the first iteration and the prediction position for the start word. The top prediction is the answer starts with \"fullback\" \n",
    "\n",
    "The second from top row denotes is the first iteration and the prediction position for the end word. The top prediction is the answer ends with \"Trevathan\".\n",
    "\n",
    "The subsequent iterations (i = 2, 3) are the next row pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dynamic Coattention Network (DCN): Mechanics\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/dpd.png\" height=\"500\"/></center>\n",
    "\n",
    "A LSTM that learns to point to the start and end of the answer (dynamic pointing decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://arxiv.org/abs/1611.01604)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Joke Interlude\n",
    "----\n",
    "<center><img src=\"http://www.cs.cornell.edu/courses/cs6742/2014fa/images/constructive-accomplished-xkcd.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the advantages for using a CNN over a RNN for NLP?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- CNNs are quicker to train. Fewer parameters.\n",
    "- Simpler. No state.\n",
    "- Somewhat interpretable.\n",
    "- RNN have to use the previous word vector.\n",
    "\n",
    "[Source](https://cs224d.stanford.edu/lectures/CS224d-Lecture13.pdf) & Edward Banner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From RNN to CNNs\n",
    "-------\n",
    "\n",
    "RNN has to use previous word (in order).\n",
    "\n",
    "Example:  \n",
    "> the country of my birth\n",
    "\n",
    "CNN can learn weights for each n-gram independently:\n",
    "> the country | country of | of my |  my  birth |    \n",
    "the country of | country  of  my |  of  my  birth |   \n",
    "the country of  my |  country of  my  birth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CNN for Text Classification\n",
    "------\n",
    "<center><img src=\"images/cnn_flat.png\" width=\"300\"/></center>\n",
    "\n",
    "The 1st layer embeds words into low-dimensional vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The next layer performs convolutions over the embedded word vectors using multiple filter sizes. For example, sliding over 3, 4 or 5 words at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, the result of the convolutional layer is max-pooled into a long feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Create a classification prediction with a softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source with TF code](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Learning Architecture for NLP\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/cnn.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Which NLP tasks would CNN be useful for? Which ones would it be __less__ useful for? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__More Useful__:\n",
    "\n",
    "Classifications tasks (Sentiment Analysis, Spam Detection or Topic Categorization) \n",
    "\n",
    "_Why?_: Classification is high-level combination of word vectors to predict labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Less Useful__:\n",
    "\n",
    "PoS Tagging or Entity Extraction:\n",
    "\n",
    "_Why?_: Convolutions and pooling operations lose information about the local order of words.\n",
    "\n",
    "[Source](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- DL is \"eating\" NLP.\n",
    "- IR is moving away from string literals towards meaning with the help of DL.\n",
    "- Question Answering might be the \"killer app\" for DL in NLP\n",
    "- CNN can be applied to NLP where the answer is combination of features.\n",
    "- Choose the right architecture for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Richard Socher\n",
    "------\n",
    "\n",
    "<center><img src=\"http://web.stanford.edu/class/cs224n/images/richard.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
